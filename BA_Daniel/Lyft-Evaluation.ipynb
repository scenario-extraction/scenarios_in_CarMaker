{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Lyft-Tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set vars\n",
    "LYFT_DATASET_ROOT = '/home/itiv/Desktop/lyft-kaggle-dataset/train/'\n",
    "CONFIG_DIR = \"./Lyft-Detector/second.pytorch/second/configs/nuscenes/\"\n",
    "MODEL_DIR = './Lyft-Detector/second.pytorch/second/model/'\n",
    "#MODEL = 'small' #small,middle,large\n",
    "VERSION = \"v1.0-trainval\" #v1.0-test for test-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 category,\n",
      "18 attribute,\n",
      "4 visibility,\n",
      "18421 instance,\n",
      "10 sensor,\n",
      "148 calibrated_sensor,\n",
      "177789 ego_pose,\n",
      "180 log,\n",
      "180 scene,\n",
      "22680 sample,\n",
      "189504 sample_data,\n",
      "638179 sample_annotation,\n",
      "1 map,\n",
      "Done loading in 7.9 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 2.2 seconds.\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "#### load ground truth data\n",
    "# Load the SDK\n",
    "%matplotlib inline\n",
    "from lyft_dataset_sdk.lyftdataset import LyftDataset\n",
    "\n",
    "# Load the dataset\n",
    "# Adjust the dataroot parameter below to point to your local dataset path.\n",
    "# The correct dataset path contains at least the following four folders (or similar): images, lidar, maps, v1.0.1-train\n",
    "level5data = LyftDataset(data_path=LYFT_DATASET_ROOT, json_path=LYFT_DATASET_ROOT+'/data', verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### create pkl files\n",
    "def create_pkl(scene_num):\n",
    "    scene = level5data.scene[scene_num]\n",
    "    scene_token = scene['token']\n",
    "    version = \"v1.0-trainval\"\n",
    "    !python Lyft-Detector/second.pytorch/second/create_data.py nuscenes_data_prep --root_path=$LYFT_DATASET_ROOT  --version=$version --dataset_name=\"NuScenesDataset\" --scene_token=$scene_token --max_sweeps=10\n",
    "\n",
    "def set_config_file(scene_num,model):\n",
    "    import numpy as np\n",
    "    from second.protos import pipeline_pb2\n",
    "    from google.protobuf import text_format\n",
    "    scene = level5data.scene[scene_num]\n",
    "    scene_token = scene['token']\n",
    "    config_path = CONFIG_DIR+'/all.pp.lowa_'+model+'_range.config'\n",
    "    info_path = LYFT_DATASET_ROOT+'/infos_'+scene_token+'.pkl'\n",
    "\n",
    "    config = pipeline_pb2.TrainEvalPipelineConfig()\n",
    "    with open(config_path, \"r\") as f:\n",
    "        proto_str = f.read()\n",
    "        text_format.Merge(proto_str, config)\n",
    "    config.train_input_reader.dataset.kitti_info_path = info_path\n",
    "    config.train_input_reader.dataset.kitti_root_path = LYFT_DATASET_ROOT\n",
    "\n",
    "    config.train_input_reader.preprocess.database_sampler.database_info_path = info_path\n",
    "\n",
    "    config.eval_input_reader.dataset.kitti_info_path = info_path\n",
    "    config.eval_input_reader.dataset.kitti_root_path = LYFT_DATASET_ROOT\n",
    "\n",
    "    config_text = text_format.MessageToString(config)\n",
    "    with open(config_path, \"w\") as f:\n",
    "        f.write(config_text)\n",
    "\n",
    "    print('Paths set in config file')\n",
    "def track(scene_num,model,net):\n",
    "    #model: small,middle,range\n",
    "    scene = level5data.scene[scene_num]\n",
    "    scene_token = scene['token']\n",
    "    config_path = CONFIG_DIR+'/all.pp.lowa_'+model+'_range.config'\n",
    "    ckpt_path= MODEL_DIR+net\n",
    "    #if model == 'small':\n",
    "    #    ckpt_path= MODEL_DIR+'/model_standard/voxelnet-48698.tckpt'\n",
    "    #elif model == 'middle':\n",
    "    #    ckpt_path = MODEL_DIR+'/model_middle_range/voxelnet-374148.tckpt'\n",
    "    #else:\n",
    "    #    ckpt_path = ''\n",
    "\n",
    "    info_path = LYFT_DATASET_ROOT+'/infos_'+scene_token+'.pkl'\n",
    "    root_path = LYFT_DATASET_ROOT\n",
    "    result_path = LYFT_DATASET_ROOT\n",
    "    version = \"v1.0-trainval\"\n",
    "\n",
    "    print('Detecting...this might take several minutes...')\n",
    "    !python ./Lyft-Detector/second.pytorch/second/detect.py detect --scene_token=$scene_token --config_path=$config_path --ckpt_path=$ckpt_path --info_path=$info_path --root_path=$root_path --result_path=$result_path\n",
    "    print('Tracking...this might take several minutes...')\n",
    "    detection_file = result_path+'/detections_'+scene_token+'.json'\n",
    "    !python ./Lyft-Tracker/mahalanobis_3d_multi_object_tracking/main.py track --save_root=$root_path --version=$version --detection_file=$detection_file --data_root=$root_path\n",
    "    \n",
    "    ## generate output\n",
    "    import json\n",
    "    import pandas as pd\n",
    "    from pyquaternion import Quaternion\n",
    "    import numpy as np\n",
    "    import math\n",
    "    import operator\n",
    "    from operator import add\n",
    "    from scipy.stats import beta\n",
    "    from collections import Counter\n",
    "    import os\n",
    "    import random\n",
    "\n",
    "    scene = level5data.scene[scene_num]\n",
    "    scene_token = scene['token']\n",
    "\n",
    "    tracking_file = LYFT_DATASET_ROOT+'/tracking_results_'+scene_token+'.json'\n",
    "\n",
    "    with open(tracking_file) as json_file:\n",
    "        data = json.load(json_file)\n",
    "\n",
    "    counter = 0\n",
    "\n",
    "    tracking_ids = []\n",
    "    sample_token = scene['first_sample_token']\n",
    "    timestamp = 0\n",
    "    for index in range(scene['nbr_samples']):\n",
    "        sample = level5data.get('sample', sample_token)\n",
    "        for detection_dict in data['results'][sample_token]:\n",
    "            if not int(detection_dict['tracking_id']) in tracking_ids:\n",
    "                tracking_ids.append(int(detection_dict['tracking_id']))\n",
    "            else:\n",
    "                counter += 1\n",
    "        sample_token = sample['next']\n",
    "        if sample_token is '':\n",
    "            break\n",
    "\n",
    "    tracking_ids.sort()            \n",
    "\n",
    "    print(f'Extracted {len(tracking_ids)} tracks')\n",
    "\n",
    "\n",
    "    index = range(scene['nbr_samples'])\n",
    "    columns = [\"Car \"+str(x) for x in tracking_ids]\n",
    "\n",
    "\n",
    "    traffic_coords = pd.DataFrame(index=index, columns=columns)\n",
    "    traffic_coords = traffic_coords.astype(object)\n",
    "\n",
    "    traffic_orientation = pd.DataFrame(index=index, columns=columns)\n",
    "    traffic_orientation = traffic_orientation.astype(object)\n",
    "\n",
    "    traffic_sizes = pd.DataFrame(index=index, columns=columns)\n",
    "    traffic_sizes = traffic_sizes.astype(object)\n",
    "\n",
    "    columns = ['Timestamp','Translation','Yaw']\n",
    "    ego_info = pd.DataFrame(index=index, columns=columns)\n",
    "    ego_info = ego_info.astype(object)\n",
    "\n",
    "\n",
    "    translation_offset = None\n",
    "\n",
    "    sample_token = scene['first_sample_token']\n",
    "    for index in range(scene['nbr_samples']):\n",
    "        sample = level5data.get('sample', sample_token)\n",
    "        sample_data = level5data.get('sample_data', sample['data']['LIDAR_TOP'])\n",
    "\n",
    "        ego_pose = level5data.get('ego_pose', sample_data['ego_pose_token'])\n",
    "\n",
    "        if translation_offset is None:\n",
    "            translation_offset = ego_pose['translation']\n",
    "\n",
    "        ego_info.at[index, 'Timestamp'] = ego_pose['timestamp']\n",
    "        ego_info.at[index, 'Yaw'] = Quaternion(ego_pose['rotation']).yaw_pitch_roll[0]\n",
    "        ego_info.at[index, 'Translation'] = list(map(operator.sub,ego_pose['translation'],translation_offset))\n",
    "        #filter pedestrians and bicycles    \n",
    "        for detection_dict in data['results'][sample_token]:\n",
    "            if not (detection_dict['tracking_name'] == 'bicycle' or detection_dict['tracking_name'] == 'pedestrian'):\n",
    "                traffic_coords.at[index, 'Car '+detection_dict['tracking_id']] =  list(map(operator.sub,detection_dict['translation'],translation_offset))\n",
    "                traffic_orientation.at[index, 'Car '+detection_dict['tracking_id']] = Quaternion(detection_dict['rotation']).yaw_pitch_roll[0]\n",
    "                traffic_sizes.at[index, 'Car '+detection_dict['tracking_id']] = detection_dict['size']\n",
    "        sample_token = sample['next']\n",
    "        if sample_token is '':\n",
    "            break\n",
    "\n",
    "    #filter short occurence traffic <1s\n",
    "    for index,entrys in traffic_coords.count().iteritems():\n",
    "        if entrys < 5:\n",
    "            traffic_coords.drop(index,axis=1,inplace=True)\n",
    "            traffic_sizes.drop(index,axis=1,inplace=True)\n",
    "            traffic_orientation.drop(index,axis=1,inplace=True)\n",
    "\n",
    "\n",
    "    def dist(l1,l2):\n",
    "        d = math.sqrt((l1[0]-l2[0])**2+(l1[1]-l2[1])**2+(l1[2]-l2[2])**2)\n",
    "        return d\n",
    "\n",
    "    #filter intersection traffic with ego\n",
    "    for (columnName, columnData) in traffic_coords.iteritems():\n",
    "        for index,item in enumerate(traffic_coords[columnName]):\n",
    "            if type(ego_info.at[index, 'Translation']) is list and type(item) is list and dist(item,ego_info.at[index, 'Translation']) < 1:\n",
    "                traffic_coords.drop(columnName,axis=1,inplace=True)\n",
    "                traffic_sizes.drop(columnName,axis=1,inplace=True)\n",
    "                traffic_orientation.drop(columnName,axis=1,inplace=True)\n",
    "                print(f'dropped {columnName}')\n",
    "                break;\n",
    "\n",
    "\n",
    "    #interpolate short NaNs\n",
    "    for (columnName, columnData) in traffic_coords.iteritems():\n",
    "        for index,item in enumerate(traffic_coords[columnName]):\n",
    "            if type(item) is list and len(traffic_coords[columnName])>index+1 and type(traffic_coords[columnName][index+1]) is not list:\n",
    "                for step in range(10):\n",
    "                    if len(traffic_coords[columnName])>index+1+step and type(traffic_coords[columnName][index+1+step]) is list:\n",
    "                        v = list(map(operator.sub, traffic_coords[columnName][index+1+step],item))\n",
    "                        traffic_coords[columnName][index+1] = list(map(operator.add, item, [x / (step+1) for x in v]))\n",
    "                        traffic_orientation[columnName][index+1] = (traffic_orientation[columnName][index+1+step]-traffic_orientation[columnName][index])/(step+1)\n",
    "                        traffic_sizes[columnName][index+1] = traffic_sizes[columnName][index]\n",
    "                        #print(f'{item} {traffic_coords[columnName][index+1]} {traffic_coords[columnName][index+1+step]}')\n",
    "                        break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def plotVal(x):\n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "        import seaborn as sns\n",
    "        import matplotlib.pyplot as plt\n",
    "        from scipy import stats\n",
    "        plt.figure(figsize=(16, 6))\n",
    "        sns.distplot(x);\n",
    "\n",
    "\n",
    "    #suppress random orientation changes \n",
    "    #driving vehicle: orientation according to direction\n",
    "    #standing vehicle: orientation according to most detected\n",
    "\n",
    "    dt = 0.2 #Periodendauer\n",
    "    for (columnName, columnData) in traffic_orientation.iteritems():\n",
    "\n",
    "        traffic_speed = []\n",
    "        moving = False\n",
    "        prev_speed = None\n",
    "        for index,item in enumerate(traffic_coords[columnName]):\n",
    "            \n",
    "            if type(item) is list and len(traffic_coords[columnName])>index+1 and type(traffic_coords[columnName][index+1]) is list:\n",
    "                traffic_speed.append(abs(dist(item,traffic_coords[columnName][index+1])/dt))\n",
    "                prev_speed = abs(dist(item,traffic_coords[columnName][index+1])/dt)\n",
    "            elif prev_speed is not None:\n",
    "                traffic_speed.append(prev_speed)\n",
    "                prev_speed = None\n",
    "            else:\n",
    "                traffic_speed.append(np.nan)\n",
    "\n",
    "        prev_orientation = None\n",
    "        not_moving_orientations = []\n",
    "        not_moving_start_index = 0\n",
    "        \n",
    "        for index,item in enumerate(traffic_speed):\n",
    "\n",
    "            if moving and (abs(item) < 1.5 or math.isnan(item)):\n",
    "                moving = False\n",
    "                not_moving_orientations = []\n",
    "                not_moving_start_index = index\n",
    "            elif not moving and (abs(item) > 2 or math.isnan(item)): \n",
    "                if len(not_moving_orientations) > 1:\n",
    "                    orientation,_ = (Counter(not_moving_orientations).most_common(1)[0])\n",
    "                    if not math.isnan(orientation):\n",
    "                        for idx in range(index-not_moving_start_index):\n",
    "                            traffic_orientation[columnName][index-idx] = orientation\n",
    "                moving = True\n",
    "                \n",
    "                \n",
    "            if not math.isnan(item):\n",
    "                if moving and index >= len(traffic_speed)-5:#ende der liste\n",
    "                    if prev_orientation is not None:\n",
    "                        traffic_orientation[columnName][index] = prev_orientation\n",
    "                elif moving and type(traffic_coords[columnName][index]) is list and type(traffic_coords[columnName][index+5]) is list:\n",
    "                    v = list(map(operator.sub,traffic_coords[columnName][index],traffic_coords[columnName][index+5]))\n",
    "                    yaw = math.atan2(v[1],v[0])-np.pi\n",
    "                    traffic_orientation[columnName][index] = yaw #---> TODO!!!!!!!!!!!!!!!!!!!!\n",
    "                elif moving and type(traffic_coords[columnName][index]) is list and type(traffic_coords[columnName][index+1]) is not list and prev_orientation is not None:\n",
    "                    traffic_orientation[columnName][index] = prev_orientation\n",
    "                elif not moving:\n",
    "                    not_moving_orientations.append(round(traffic_orientation[columnName][index],1))\n",
    "\n",
    "                prev_orientation = traffic_orientation[columnName][index]\n",
    "            else:\n",
    "                traffic_orientation[columnName][index] = prev_orientation\n",
    "                \n",
    "        if len(not_moving_orientations) > 1:\n",
    "            orientation,_ = (Counter(not_moving_orientations).most_common(1)[0])\n",
    "            if not math.isnan(orientation):\n",
    "                for idx in range(len(traffic_speed)-not_moving_start_index-1):\n",
    "                    traffic_orientation[columnName][index-idx] = orientation\n",
    "\n",
    "    ## create result_json_dict #####\n",
    "    \n",
    "    \n",
    "    print(traffic_orientation)\n",
    "    print(traffic_coords)\n",
    "\n",
    "\n",
    "    gt_dict = {'samples':[]}\n",
    "\n",
    "    #iterate over rows\n",
    "    for index, row in traffic_coords.iterrows():\n",
    "        #iterate over columns\n",
    "        sample_dict = {'token':'frame_'+str(index),'timestamp':index*0.2,'anns':[]}\n",
    "\n",
    "        for colIndex in range(len(row)):\n",
    "            if type(traffic_coords.iloc[index,colIndex]) is list:\n",
    "                ann_dict = {'instance':colIndex,'translation':traffic_coords.iloc[index,colIndex],'size':traffic_sizes.iloc[index,colIndex],'rotation':traffic_orientation.iloc[index,colIndex]}\n",
    "                sample_dict['anns'].append(ann_dict)\n",
    "        gt_dict['samples'].append(sample_dict)\n",
    "\n",
    "\n",
    "    def dist(l1,l2):\n",
    "        d = math.sqrt((l1[0]-l2[0])**2+(l1[1]-l2[1])**2+(l1[2]-l2[2])**2)\n",
    "        return d\n",
    "    \n",
    "    ##########\n",
    "\n",
    "    CM_PROJECT_DIR =  LYFT_DATASET_ROOT+'/CarMaker'#'C://CM_Projects//maneuver_simu'\n",
    "    TESTRUN_DIR = CM_PROJECT_DIR+'/TestRun'#CM_PROJECT_DIR+'//Data//TestRun'\n",
    "    SIMINPUT_DIR = CM_PROJECT_DIR+'/SimInput/'#CM_PROJECT_DIR+'//SimInput'\n",
    "\n",
    "    GROUND_TRUTH_DIR = CM_PROJECT_DIR+'/gt_data/'  #only for eval\n",
    "\n",
    "    if not os.path.exists(CM_PROJECT_DIR):\n",
    "        os.mkdir(CM_PROJECT_DIR)\n",
    "    if not os.path.exists(TESTRUN_DIR):\n",
    "        os.mkdir(TESTRUN_DIR)\n",
    "    if not os.path.exists(SIMINPUT_DIR):\n",
    "        os.mkdir(SIMINPUT_DIR)\n",
    "    if not os.path.exists(GROUND_TRUTH_DIR):\n",
    "        os.mkdir(GROUND_TRUTH_DIR)\n",
    "\n",
    "    SCENE_NUM = scene_num\n",
    "\n",
    "    TESTRUN_ID = 'scene-'+str(SCENE_NUM)+'-'+model\n",
    "    TESTRUN_NAME = f'//testrun_{TESTRUN_ID}'\n",
    "    TESTRUN_PATH = TESTRUN_DIR+TESTRUN_NAME\n",
    "    TRAFFIC_PROFILE_NAME = f'traffic_profile_{TESTRUN_ID}.txt'\n",
    "    TRAFFIC_PROFILE_PATH = SIMINPUT_DIR+TRAFFIC_PROFILE_NAME\n",
    "    EGO_PROFILE_NAME = f'ego_profile_{TESTRUN_ID}.txt'\n",
    "    EGO_PROFILE_PATH = SIMINPUT_DIR+EGO_PROFILE_NAME\n",
    "\n",
    "    gt_data_name = f'gt_data_{TESTRUN_ID}.json'\n",
    "    with open(GROUND_TRUTH_DIR+gt_data_name, 'w') as fp:\n",
    "        print('OPENED '+GROUND_TRUTH_DIR+gt_data_name)\n",
    "        json.dump(gt_dict, fp)\n",
    "\n",
    "\n",
    "\n",
    "    content_lines = []\n",
    "    content_lines.append('#INFOFILE1.1 - Do not remove this line!\\n\\\n",
    "FileIdent = CarMaker-TestRun 8\\n\\\n",
    "FileCreator = CarMaker 8.1 2019-11-07\\n\\\n",
    "Description:\\n\\\n",
    "Vehicle = UserVehicle_MyCar\\n\\\n",
    "Trailer =\\n\\\n",
    "Tire.0 =\\n\\\n",
    "Tire.1 =\\n\\\n",
    "Tire.2 =\\n\\\n",
    "Tire.3 =\\n\\\n",
    "Snapshot.TimeLimit =\\n\\\n",
    "Snapshot.DistLimit =\\n\\\n",
    "VehicleLoad.0.mass = 0\\n\\\n",
    "VehicleLoad.0.pos = 0 0 0\\n\\\n",
    "VehicleLoad.1.mass = 0\\n\\\n",
    "VehicleLoad.1.pos = 0 0 0\\n\\\n",
    "VehicleLoad.2.mass = 0\\n\\\n",
    "VehicleLoad.2.pos = 0 0 0\\n\\\n",
    "VehicleLoad.3.mass = 0\\n\\\n",
    "VehicleLoad.3.pos = 0 0 0\\n\\\n",
    "TrailerLoad.0.mass = 0\\n\\\n",
    "TrailerLoad.0.pos = 0 0 0\\n\\\n",
    "TrailerLoad.1.mass = 0\\n\\\n",
    "TrailerLoad.1.pos = 0 0 0\\n\\\n",
    "TrailerLoad.2.mass = 0\\n\\\n",
    "TrailerLoad.2.pos = 0 0 0\\n')\n",
    "\n",
    "    #ego maneuver for every 10th coord\n",
    "\n",
    "    time_between_records = 0.2 \n",
    "\n",
    "    start_vel = 0#ego_vel[0]\n",
    "    man_counter = 1\n",
    "    #ego_man = []\n",
    "    #for vel in ego_vel[::10]:\n",
    "    #    ego_man.append('DrivMan.'+str(man_counter)+'.TimeLimit = '+str(10*time_between_records)+'\\n\\\n",
    "    #DrivMan.'+str(man_counter)+'.LongDyn = VelControl '+str(vel*3.6)+' 0.0 1.0 0 1 0\\n\\\n",
    "    #DrivMan.'+str(man_counter)+'.LatDyn = Driver 0\\n')\n",
    "    #    man_counter += 1\n",
    "\n",
    "           # +str(start_vel)+\n",
    "    content_lines.append('DrivMan.Init.Velocity = 0\\n\\\n",
    "DrivMan.Init.SteerAng = 0\\n\\\n",
    "DrivMan.Init.GearNo = 0\\n\\\n",
    "DrivMan.Init.LaneOffset = 0\\n\\\n",
    "DrivMan.Init.OperatorActive = 1\\n\\\n",
    "DrivMan.Init.OperatorState = drive\\n\\\n",
    "DrivMan.VhclOperator.Kind = IPGOperator 1\\n\\\n",
    "DrivMan.nDMan = '+str(man_counter)+'\\n')\n",
    "\n",
    "    content_lines.append('DrivMan.0.TimeLimit = '+str(time_between_records*(len(ego_info)-1))+'\\n\\\n",
    "DrivMan.0.LongDyn = Stop 2.0 0\\n\\\n",
    "DrivMan.0.LatDyn = Driver 0\\n')\n",
    "\n",
    "\n",
    "    #write traffic profile file\n",
    "\n",
    "    traffic_profile = []\n",
    "    traffic_profile.append('# Time')\n",
    "    for index,col in enumerate(traffic_coords):\n",
    "        traffic_profile.append(' FM_tx_%d FM_ty_%d FM_tz_%d FM_rz_%d' % (index,index,index,index))\n",
    "    traffic_profile.append('\\n')    \n",
    "\n",
    "\n",
    "    prev_row = None\n",
    "    for time_index, row in traffic_coords.iterrows():\n",
    "        traffic_profile.append('%f'%(time_index*time_between_records))\n",
    "        for car_index,coords in enumerate(row):\n",
    "            if type(coords) is list:\n",
    "                if(time_index == None or car_index == None):\n",
    "                    print('index error')\n",
    "                    traffic_profile.append(' %f %f %f %f'%(0,0,-100,0))\n",
    "                elif(traffic_orientation.iloc[time_index,car_index] == None):\n",
    "                    print('#####orientation error!!!')\n",
    "                    traffic_profile.append(' %f %f %f %f'%(0,0,-100,0))\n",
    "                else:\n",
    "                    traffic_profile.append(' %f %f %f %f'%(coords[0],coords[1],0.6,traffic_orientation.iloc[time_index,car_index]))\n",
    "            else:\n",
    "                traffic_profile.append(' %f %f %f %f'%(0,0,-100,0))\n",
    "\n",
    "                \n",
    "        prev_row = row\n",
    "        traffic_profile.append('\\n') \n",
    "\n",
    "    with open(TRAFFIC_PROFILE_PATH, 'w') as f:\n",
    "        for item in traffic_profile:\n",
    "            f.write(\"%s\" % item)\n",
    "\n",
    "\n",
    "    traffic_counter = 0   \n",
    "    traffic_counter = len(traffic_coords.columns)\n",
    "    content_lines.append('Traffic.IFF.FName = SimInput/'+TRAFFIC_PROFILE_NAME+'\\n\\\n",
    "Traffic.IFF.Time.Name = Time\\n\\\n",
    "Traffic.N = %d\\n\\\n",
    "Traffic.SpeedUnit = ms\\n'%traffic_counter)\n",
    "\n",
    "\n",
    "    #get average sizes of vehicle\n",
    "\n",
    "    traffic_sizes_avr = []\n",
    "    for (columnName, columnData) in traffic_sizes.iteritems():\n",
    "        counter = 0\n",
    "        l_sum = 0\n",
    "        for size in columnData.values:\n",
    "            if isinstance(size, list):\n",
    "                counter += 1\n",
    "                l_sum += size[1]\n",
    "        traffic_sizes_avr.append(l_sum/counter)\n",
    "\n",
    "    small_car = ['Audi_TT_2015.mobj','Citroen_C3_2015.mobj','Honda_Fit_2015.mobj'] #l<4m\n",
    "    medium_car = ['Audi_A4AllRoad_2016.mobj','Audi_A7_2018.mobj','BMW_5_2017.mobj','Honda_CivicTypeR_2018.mobj','MB_AClass_2018.mobj','MB_CClass_2015.mobj','Jaguar_FType_2017.mobj'] #l<5m\n",
    "    large_car = ['Chevrolet_Silverado1500_2013.mobj','Chrysler_Pacifica_2016.mobj','Dodge_GrandCaravan_2011.mobj','LandRover_RangeRover_2014.mobj','MB_XClass_2018.mobj'] #l<6m\n",
    "    van = ['Ford_Transit_2014.mobj','MB_Sprinter_2013.mobj','MB_Vito_2014.mobj','VW_T6_2016.mobj','VW_Transporter_2016.mobj'] #l<10m\n",
    "    bus_truck = ['Coach.mobj','Euro_ConcreteMixer.mobj','Iveco_EurotechLN2_1992.mobj','MAN_TGS_2012.mobj','MB_Actros_1996.mobj','MB_Atego_2013_Move.mobj'] #l>10m\n",
    "\n",
    "\n",
    "\n",
    "    for index,col in enumerate(traffic_coords): \n",
    "\n",
    "        vehicle_path = '3D/Vehicles/'\n",
    "\n",
    "        if traffic_sizes_avr[index]<4:\n",
    "            vehicle_path = vehicle_path+small_car[random.randrange(0,len(small_car),1)]\n",
    "        elif traffic_sizes_avr[index]<5:\n",
    "            vehicle_path = vehicle_path+medium_car[random.randrange(0,len(medium_car),1)]\n",
    "        elif traffic_sizes_avr[index]<6:\n",
    "            vehicle_path = vehicle_path+large_car[random.randrange(0,len(large_car),1)]\n",
    "        elif traffic_sizes_avr[index]<10:\n",
    "            vehicle_path = vehicle_path+van[random.randrange(0,len(van),1)]   \n",
    "        elif traffic_sizes_avr[index]>10:\n",
    "            vehicle_path = vehicle_path+bus_truck[random.randrange(0,len(bus_truck),1)]   \n",
    "\n",
    "        content_lines.append('Traffic.'+str(index)+'.ObjectKind = Movable\\n\\\n",
    "Traffic.'+str(index)+'.ObjectClass = Car\\n\\\n",
    "Traffic.'+str(index)+'.Name = T'+str(index)+'\\n\\\n",
    "Traffic.'+str(index)+'.Info = UNNAMED Object '+str(index)+'\\n\\\n",
    "Traffic.'+str(index)+'.Movie.Geometry = '+vehicle_path+'\\n\\\n",
    "Traffic.'+str(index)+'.Color = 1.0 0.0 0.0\\n\\\n",
    "Traffic.'+str(index)+'.Basics.Dimension = 4.28 1.82 1.28\\n\\\n",
    "Traffic.'+str(index)+'.Basics.Offset = 0.0 0.0\\n\\\n",
    "Traffic.'+str(index)+'.Basics.Fr12CoM = 2.15\\n\\\n",
    "Traffic.'+str(index)+'.Init.Orientation = 0.0 0.0 0.0\\n\\\n",
    "Traffic.'+str(index)+'.RCSClass = RCS_Car\\n\\\n",
    "Traffic.'+str(index)+'.DetectMask = 1 1\\n\\\n",
    "Traffic.'+str(index)+'.Route = 0 1\\n\\\n",
    "Traffic.'+str(index)+'.Init.Road = 18 R2\\n\\\n",
    "Traffic.'+str(index)+'.Init.v = 1\\n\\\n",
    "Traffic.'+str(index)+'.FreeMotion = 1\\n\\\n",
    "Traffic.'+str(index)+'.UpdRate = 200\\n\\\n",
    "Traffic.'+str(index)+'.IFF.FM_tx.Name =FM_tx_'+str(index)+'\\n\\\n",
    "Traffic.'+str(index)+'.IFF.FM_tx.Factor = 1.0\\n\\\n",
    "Traffic.'+str(index)+'.IFF.FM_tx.Offset = 0.0\\n\\\n",
    "Traffic.'+str(index)+'.IFF.FM_ty.Name =FM_ty_'+str(index)+'\\n\\\n",
    "Traffic.'+str(index)+'.IFF.FM_ty.Factor = 1.0\\n\\\n",
    "Traffic.'+str(index)+'.IFF.FM_ty.Offset = 0.0\\n\\\n",
    "Traffic.'+str(index)+'.IFF.FM_tz.Name =FM_tz_'+str(index)+'\\n\\\n",
    "Traffic.'+str(index)+'.IFF.FM_tz.Factor = 1.0\\n\\\n",
    "Traffic.'+str(index)+'.IFF.FM_tz.Offset = 0.0\\n\\\n",
    "Traffic.'+str(index)+'.IFF.FM_rx.Name =\\n\\\n",
    "Traffic.'+str(index)+'.IFF.FM_rx.Factor = 1.0\\n\\\n",
    "Traffic.'+str(index)+'.IFF.FM_rx.Offset = 0.0\\n\\\n",
    "Traffic.'+str(index)+'.IFF.FM_ry.Name =\\n\\\n",
    "Traffic.'+str(index)+'.IFF.FM_ry.Factor = 1.0\\n\\\n",
    "Traffic.'+str(index)+'.IFF.FM_ry.Offset = 0.0\\n\\\n",
    "Traffic.'+str(index)+'.IFF.FM_rz.Name =FM_rz_'+str(index)+'\\n\\\n",
    "Traffic.'+str(index)+'.IFF.FM_rz.Factor = 1.0\\n\\\n",
    "Traffic.'+str(index)+'.IFF.FM_rz.Offset = 0.0\\n')\n",
    "\n",
    "\n",
    "    ego_coords = ego_info['Translation'].tolist()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    road_points = []\n",
    "\n",
    "    prev_row = None\n",
    "    distance = 5.5\n",
    "    for index,row in enumerate(ego_coords[::1]):  \n",
    "        if prev_row is None or abs(dist([row[0],row[1],0],prev_row))>1 :\n",
    "            dx = -1*math.sin(ego_info.at[index, 'Yaw'])*distance\n",
    "            dy = math.cos(ego_info.at[index, 'Yaw'])*distance\n",
    "            road_points.append([row[0]+dx,row[1]+dy])\n",
    "            prev_row=[row[0],row[1],0]\n",
    "\n",
    "\n",
    "\n",
    "    #pointlist start degree\n",
    "    dx = road_points[1][0]-road_points[0][0]\n",
    "    dy = road_points[1][1]-road_points[0][1]\n",
    "\n",
    "    startDeg = math.degrees(math.atan2(dx, dy))\n",
    "\n",
    "\n",
    "    #pointlist end vector\n",
    "    dx = ego_coords[-1][0]-ego_coords[-2][0]\n",
    "    dy = ego_coords[-1][1]-ego_coords[-2][1]\n",
    "\n",
    "    content_lines.append('DrivMan.OW.Active = 1\\n\\\n",
    "DrivMan.OW.Quantities = Time User1 User2 User3 User4\\n\\\n",
    "DrivMan.OW.StartGearNo = 1\\n\\\n",
    "DrivMan.OW.StartVelocity =\\n\\\n",
    "DrivMan.OW.GasMax = 0.5\\n\\\n",
    "DrivMan.OW.RefCh = Time\\n\\\n",
    "DrivMan.OW.ConsiderRoadSigns = 0\\n\\\n",
    "DrivMan.OW.sRoute.Offset = 0\\n\\\n",
    "DrivMan.OW.Time.Name = t[s]\\n\\\n",
    "DrivMan.OW.Time.Factor = 1.0\\n\\\n",
    "DrivMan.OW.User1.Name = x\\n\\\n",
    "DrivMan.OW.User1.Factor = 1.0\\n\\\n",
    "DrivMan.OW.User1.Offset = 0.0\\n\\\n",
    "DrivMan.OW.User2.Name = y\\n\\\n",
    "DrivMan.OW.User2.Factor = 1.0\\n\\\n",
    "DrivMan.OW.User2.Offset = 0.0\\n\\\n",
    "DrivMan.OW.User3.Name = yaw\\n\\\n",
    "DrivMan.OW.User3.Factor = 1.0\\n\\\n",
    "DrivMan.OW.User3.Offset = 0.0\\n\\\n",
    "DrivMan.OW.User4.Name = -\\n\\\n",
    "DrivMan.OW.User4.Factor = 1.0\\n\\\n",
    "DrivMan.OW.User4.Offset = 0.0\\n\\\n",
    "DrivMan.OW.FName = '+EGO_PROFILE_NAME+'\\n\\\n",
    "ErrorClass.0.Action = abort\\n\\\n",
    "ErrorClass.0.Save = 0\\n\\\n",
    "ErrorClass.0.WarningLimit = 3 5\\n\\\n",
    "ErrorClass.1.Action = abort\\n\\\n",
    "ErrorClass.1.Save = 0\\n\\\n",
    "ErrorClass.1.WarningLimit = 3 5\\n\\\n",
    "ErrorClass.2.Action = abort\\n\\\n",
    "ErrorClass.2.Save = 0\\n\\\n",
    "ErrorClass.2.WarningLimit = 3 5\\n\\\n",
    "ErrorClass.3.Action = abort\\n\\\n",
    "ErrorClass.3.Save = 0\\n\\\n",
    "ErrorClass.3.WarningLimit = 3 5\\n\\\n",
    "ErrorClass.4.Action = abort\\n\\\n",
    "ErrorClass.4.Save = 0\\n\\\n",
    "ErrorClass.4.WarningLimit = 3 5\\n\\\n",
    "ErrorClass.5.Action = abort\\n\\\n",
    "ErrorClass.5.Save = 0\\n\\\n",
    "ErrorClass.5.WarningLimit = 3 5\\n\\\n",
    "ErrorClass.6.Action = abort\\n\\\n",
    "ErrorClass.6.Save = 0\\n\\\n",
    "ErrorClass.6.WarningLimit = 10 5\\n\\\n",
    "ErrorClass.7.Action = abort\\n\\\n",
    "ErrorClass.7.Save = 0\\n\\\n",
    "ErrorClass.7.WarningLimit = 3 5\\n\\\n",
    "ErrorClass.10.Action = abort\\n\\\n",
    "ErrorClass.10.Save = 0\\n\\\n",
    "ErrorClass.10.WarningLimit = 3 5\\n\\\n",
    "ErrorClass.11.Action = abort\\n\\\n",
    "ErrorClass.11.Save = 0\\n\\\n",
    "ErrorClass.11.WarningLimit = 3 5\\n\\\n",
    "Road.FileIdent = IPGRoad 8.0\\n\\\n",
    "Road.LibVersion = 8.1\\n\\\n",
    "Road.Country = DEU\\n\\\n",
    "Road.nLinks = 1\\n\\\n",
    "Road.nJunctions = 0\\n\\\n",
    "Road.nObjects = 149\\n\\\n",
    "Road.nRoutes = 1\\n\\\n",
    "Road.RoadNetworkLength = 595.296984550432\\n\\\n",
    "Road.BBox = -19.5668198180623 547.415304391587 -224.837648089968 34.6179698226834 -11 11\\n\\\n",
    "Road.Route.0.Length = 595.296984550432\\n\\\n",
    "Road.RST.Unit = kmh\\n\\\n",
    "Road.RST = 50 100 130 30 70 30 0 -1\\n\\\n",
    "Road.Movie = 0.2 1 0.01 1.5 1.5 1 1\\n\\\n",
    "Road.PathMode = -1\\n\\\n",
    "Road.Link.0.ID = 0\\n\\\n",
    "Road.Link.0.Junctions = -1 -1 -2 -1\\n\\\n",
    "Road.Link.0.Node0 = 0 0 0 '+str(startDeg)+'\\n\\\n",
    "Road.Link.0.RST = countryroad\\n\\\n",
    "Road.Link.0.RL.ID = 1\\n\\\n",
    "Road.Link.0.Seg.0.ID = 5\\n\\\n",
    "Road.Link.0.Seg.0.Type = PointList\\n\\\n",
    "Road.Link.0.Seg.0.Param =  '+str(dx)+' '+str(dy)+' 1 0 0 0 0 0\\n\\\n",
    "Road.Link.0.Seg.0.PointList:\\n')\n",
    "\n",
    "    #append road pointlist\n",
    "    obj_id=5\n",
    "    pointlist = []\n",
    "    pointlist.append('# t[s] x y yaw -\\n')\n",
    "    counter = 0\n",
    "    for index,row in enumerate(ego_coords[::1]):\n",
    "        pointlist.append('  %f %f %f %f %f\\n'%(counter*time_between_records,row[0],row[1],ego_info.at[index, 'Yaw'],0))\n",
    "        counter += 1\n",
    "\n",
    "\n",
    "    with open(EGO_PROFILE_PATH, 'w') as f:\n",
    "        for item in pointlist:\n",
    "            f.write(\"%s\" % item)\n",
    "\n",
    "\n",
    "    pointlist = ''\n",
    "\n",
    "\n",
    "\n",
    "    ##extending road\n",
    "    d0 = 500 #road extension distance\n",
    "\n",
    "\n",
    "    # at the end\n",
    "    #m = (road_points[-1][1]-road_points[-2][1])/(road_points[-1][0]-road_points[-2][0])\n",
    "    dx = road_points[-2][0]-road_points[-1][0]#dx = d0/math.sqrt(1+m**2)\n",
    "    dy = road_points[-2][1]-road_points[-1][1]#dy = m*dx\n",
    "    road_points.append([road_points[-1][0]-dx*100,road_points[-1][1]-dy*100])\n",
    "\n",
    "    # at the beginning\n",
    "    #m = (road_points[0][1]-road_points[1][1])/(road_points[0][0]-road_points[1][0])\n",
    "    dx = road_points[0][0]-road_points[1][0]#d0/math.sqrt(1+m**2)\n",
    "    dy = road_points[0][1]-road_points[1][1]#m*dx\n",
    "    #road_points = [[road_points[0][0]+dx*100,road_points[0][1]+dy*100]] + road_points\n",
    "\n",
    "    for row in road_points:\n",
    "        obj_id +=  1\n",
    "        pointlist = pointlist + '\t'+str(row[0])+' '+str(row[1])+'\\n'\n",
    "\n",
    "    content_lines.append(pointlist)    \n",
    "\n",
    "    content_lines.append('Road.Link.0.LaneSection.0.ID = '+str(obj_id+1)+'\\n\\\n",
    "Road.Link.0.LaneSection.0.Start = 0\\n\\\n",
    "Road.Link.0.LaneSection.0.LaneL.0.ID = '+str(obj_id+2)+'\\n\\\n",
    "Road.Link.0.LaneSection.0.LaneL.0 = 0 3.5 3.5 0 0 0 0\\n\\\n",
    "Road.Link.0.LaneSection.0.LaneL.0.ARP = '+str(obj_id+5)+' '+str(obj_id+6)+' '+str(obj_id+7)+' '+str(obj_id+8)+' '+str(obj_id+9)+' '+str(obj_id+10)+'\\n\\\n",
    "Road.Link.0.LaneSection.0.LaneL.1.ID = '+str(obj_id+13)+'\\n\\\n",
    "Road.Link.0.LaneSection.0.LaneL.1 = 0 0.75 0.75 4 0 0 0\\n\\\n",
    "Road.Link.0.LaneSection.0.LaneL.1.ARP = '+str(obj_id+16)+' '+str(obj_id+17)+' '+str(obj_id+18)+' '+str(obj_id+19)+' '+str(obj_id+20)+' '+str(obj_id+21)+'\\n\\\n",
    "Road.Link.0.LaneSection.0.LaneL.2.ID = '+str(obj_id+24)+'\\n\\\n",
    "Road.Link.0.LaneSection.0.LaneL.2 = 0 1.5 1.5 4 0 0 0\\n\\\n",
    "Road.Link.0.LaneSection.0.LaneL.2.ARP = '+str(obj_id+27)+' '+str(obj_id+28)+' '+str(obj_id+29)+' '+str(obj_id+30)+' '+str(obj_id+31)+' '+str(obj_id+32)+'\\n\\\n",
    "Road.Link.0.LaneSection.0.LaneL.3.ID = '+str(obj_id+34)+'\\n\\\n",
    "Road.Link.0.LaneSection.0.LaneL.3 = 0 1.5 1.5 4 0 0 0\\n\\\n",
    "Road.Link.0.LaneSection.0.LaneL.3.ARP = '+str(obj_id+37)+' '+str(obj_id+38)+' '+str(obj_id+39)+' '+str(obj_id+40)+' '+str(obj_id+41)+' '+str(obj_id+51)+'\\n\\\n",
    "Road.Link.0.LaneSection.0.LaneL.4.ID = '+str(obj_id+44)+'\\n\\\n",
    "Road.Link.0.LaneSection.0.LaneL.4 = 0 0.75 0.75 4 0 0 0\\n\\\n",
    "Road.Link.0.LaneSection.0.LaneL.4.ARP = '+str(obj_id+47)+' '+str(obj_id+48)+' '+str(obj_id+49)+' '+str(obj_id+50)+' '+str(obj_id+51)+' '+str(obj_id+52)+'\\n\\\n",
    "Road.Link.0.LaneSection.0.LaneL.5.ID = '+str(obj_id+55)+'\\n\\\n",
    "Road.Link.0.LaneSection.0.LaneL.5 = 0 3.5 3.5 0 0 0 0\\n\\\n",
    "Road.Link.0.LaneSection.0.LaneL.5.ARP = '+str(obj_id+58)+' '+str(obj_id+59)+' '+str(obj_id+60)+' '+str(obj_id+61)+' '+str(obj_id+62)+' '+str(obj_id+63)+'\\n\\\n",
    "Road.Link.0.LaneSection.0.LaneL.6.ID = '+str(obj_id+66)+'\\n\\\n",
    "Road.Link.0.LaneSection.0.LaneL.6 = 0 3.5 3.5 0 0 0 0\\n\\\n",
    "Road.Link.0.LaneSection.0.LaneL.6.ARP = '+str(obj_id+69)+' '+str(obj_id+70)+' '+str(obj_id+71)+' '+str(obj_id+72)+' '+str(obj_id+73)+' '+str(obj_id+74)+'\\n\\\n",
    "Road.Link.0.LaneSection.0.LaneL.7.ID = '+str(obj_id+77)+'\\n\\\n",
    "Road.Link.0.LaneSection.0.LaneL.7 = 0 3.5 3.5 0 0 0 0\\n\\\n",
    "Road.Link.0.LaneSection.0.LaneL.7.ARP = '+str(obj_id+80)+' '+str(obj_id+81)+' '+str(obj_id+82)+' '+str(obj_id+83)+' '+str(obj_id+84)+' '+str(obj_id+85)+'\\n\\\n",
    "Road.Link.0.LaneSection.0.LaneL.8.ID = '+str(obj_id+88)+'\\n\\\n",
    "Road.Link.0.LaneSection.0.LaneL.8 = 0 1 1 4 0 0 0\\n\\\n",
    "Road.Link.0.LaneSection.0.LaneL.8.ARP = '+str(obj_id+91)+' '+str(obj_id+92)+' '+str(obj_id+93)+' '+str(obj_id+94)+' '+str(obj_id+95)+' '+str(obj_id+96)+'\\n\\\n",
    "Road.Link.0.LaneSection.0.LaneL.9.ID = '+str(obj_id+98)+'\\n\\\n",
    "Road.Link.0.LaneSection.0.LaneL.9 = 0 2.5 2.5 5 0 0 0\\n\\\n",
    "Road.Link.0.LaneSection.0.LaneR.0.ID = '+str(obj_id+101)+'\\n\\\n",
    "Road.Link.0.LaneSection.0.LaneR.0 = 0 3.5 3.5 0 0 0 0\\n\\\n",
    "Road.Link.0.LaneSection.0.LaneR.0.ARP = '+str(obj_id+104)+' '+str(obj_id+105)+' '+str(obj_id+106)+' '+str(obj_id+107)+' '+str(obj_id+108)+' '+str(obj_id+109)+'\\n\\\n",
    "Road.Link.0.LaneSection.0.LaneR.1.ID = '+str(obj_id+112)+'\\n\\\n",
    "Road.Link.0.LaneSection.0.LaneR.1 = 0 3.5 3.5 0 0 0 0\\n\\\n",
    "Road.Link.0.LaneSection.0.LaneR.1.ARP = '+str(obj_id+115)+' '+str(obj_id+116)+' '+str(obj_id+117)+' '+str(obj_id+118)+' '+str(obj_id+119)+' '+str(obj_id+120)+'\\n\\\n",
    "Road.Link.0.LaneSection.0.LaneR.2.ID = '+str(obj_id+123)+'\\n\\\n",
    "Road.Link.0.LaneSection.0.LaneR.2 = 0 1 1 4 0 0 0\\n\\\n",
    "Road.Link.0.LaneSection.0.LaneR.2.ARP = '+str(obj_id+126)+' '+str(obj_id+127)+' '+str(obj_id+128)+' '+str(obj_id+129)+' '+str(obj_id+130)+' '+str(obj_id+131)+'\\n\\\n",
    "Road.Link.0.LaneSection.0.LaneR.3.ID = '+str(obj_id+133)+'\\n\\\n",
    "Road.Link.0.LaneSection.0.LaneR.3 = 0 2.5 2.5 5 0 0 0\\n\\\n",
    "Road.LanePath.0 = '+str(obj_id+11)+' '+str(obj_id+2)+' 2 10 0.1 0.1\\n\\\n",
    "Road.LanePath.1 = '+str(obj_id+22)+' '+str(obj_id+13)+' 0.25 10 0.1 0.1\\n\\\n",
    "Road.LanePath.2 = '+str(obj_id+33)+' '+str(obj_id+24)+' 0.25 10 0.1 0.1\\n\\\n",
    "Road.LanePath.3 = '+str(obj_id+43)+' '+str(obj_id+34)+' 0.25 10 0.1 0.1\\n\\\n",
    "Road.LanePath.4 = '+str(obj_id+53)+' '+str(obj_id+44)+' 0.25 10 0.1 0.1\\n\\\n",
    "Road.LanePath.5 = '+str(obj_id+64)+' '+str(obj_id+55)+' 2 10 0.1 0.1\\n\\\n",
    "Road.LanePath.6 = '+str(obj_id+75)+' '+str(obj_id+66)+' 2 10 0.1 0.1\\n\\\n",
    "Road.LanePath.7 = '+str(obj_id+86)+' '+str(obj_id+77)+' 2 10 0.1 0.1\\n\\\n",
    "Road.LanePath.8 = '+str(obj_id+97)+' '+str(obj_id+88)+' 0.25 10 0.1 0.1\\n\\\n",
    "Road.LanePath.9 = '+str(obj_id+110)+' '+str(obj_id+101)+' 2 10 0.1 0.1\\n\\\n",
    "Road.LanePath.10 = '+str(obj_id+121)+' '+str(obj_id+112)+' 2 10 0.1 0.1\\n\\\n",
    "Road.LanePath.11 = '+str(obj_id+132)+' '+str(obj_id+123)+' 0.25 10 0.1 0.1\\n\\\n",
    "Route.0.ID = '+str(obj_id+137)+'\\n\\\n",
    "Route.0.Name = Route_0\\n\\\n",
    "Route.0.DrvPath.ID = '+str(obj_id+138)+'\\n\\\n",
    "Route.0.DrvPath:\\n\\\n",
    "    '+str(obj_id+110)+'\\n\\\n",
    "Road.RL.1.RoadMarking.0.ID = '+str(obj_id+136)+' '+str(obj_id+1)+'\\n\\\n",
    "Road.RL.1.RoadMarking.0 = 0 0 0 1 0 0 0.15 0 2 0 0 2 4 1 1 0 \"\"\\n\\\n",
    "Road.RL.1.RoadMarking.0.Material.0 = 1.0,1.0,1.0 0 0 0 0 0 0 0 0 0 0 0\\n\\\n",
    "Road.RL.1.RoadMarking.1.ID = '+str(obj_id+122)+' '+str(obj_id+112)+'\\n\\\n",
    "Road.RL.1.RoadMarking.1 = 0 0 0 1 0 -1 0.15 0 1 0 0 2 4 1 1 0 \"\"\\n\\\n",
    "Road.RL.1.RoadMarking.1.Material.0 = 1.0,1.0,1.0 0 0 0 0 0 0 0 0 0 0 0\\n\\\n",
    "Road.RL.1.RoadMarking.2.ID = '+str(obj_id+111)+' '+str(obj_id+101)+'\\n\\\n",
    "Road.RL.1.RoadMarking.2 = 0 0 0 1 0 -1 0.15 0 2 0 0 2 4 1 1 0 \"\"\\n\\\n",
    "Road.RL.1.RoadMarking.2.Material.0 = 1.0,1.0,1.0 0 0 0 0 0 0 0 0 0 0 0\\n\\\n",
    "Road.RL.1.RoadMarking.3.ID = '+str(obj_id+87)+' '+str(obj_id+77)+'\\n\\\n",
    "Road.RL.1.RoadMarking.3 = 0 0 0 1 0 1 0.15 0 1 0 0 2 4 1 1 0 \"\"\\n\\\n",
    "Road.RL.1.RoadMarking.3.Material.0 = 1.0,1.0,1.0 0 0 0 0 0 0 0 0 0 0 0\\n\\\n",
    "Road.RL.1.RoadMarking.4.ID = '+str(obj_id+76)+' '+str(obj_id+66)+'\\n\\\n",
    "Road.RL.1.RoadMarking.4 = 0 0 0 1 0 1 0.15 0 2 0 0 2 4 1 1 0 \"\"\\n\\\n",
    "Road.RL.1.RoadMarking.4.Material.0 = 1.0,1.0,1.0 0 0 0 0 0 0 0 0 0 0 0\\n\\\n",
    "Road.RL.1.RoadMarking.5.ID = '+str(obj_id+65)+' '+str(obj_id+55)+'\\n\\\n",
    "Road.RL.1.RoadMarking.5 = 0 0 0 1 0 1 0.15 0 2 0 0 2 4 1 1 0 \"\"\\n\\\n",
    "Road.RL.1.RoadMarking.5.Material.0 = 1.0,1.0,1.0 0 0 0 0 0 0 0 0 0 0 0\\n\\\n",
    "Road.RL.1.RoadMarking.6.ID = '+str(obj_id+54)+' '+str(obj_id+44)+'\\n\\\n",
    "Road.RL.1.RoadMarking.6 = 0 0 0 1 0 1 0.15 0 1 0 0 2 4 1 1 0 \"\"\\n\\\n",
    "Road.RL.1.RoadMarking.6.Material.0 = 1.0,1.0,1.0 0 0 0 0 0 0 0 0 0 0 0\\n\\\n",
    "Road.RL.1.RoadMarking.7.ID = '+str(obj_id+23)+' '+str(obj_id+13)+'\\n\\\n",
    "Road.RL.1.RoadMarking.7 = 0 0 0 1 0 1 0.15 0 1 0 0 2 4 1 1 0 \"\"\\n\\\n",
    "Road.RL.1.RoadMarking.7.Material.0 = 1.0,1.0,1.0 0 0 0 0 0 0 0 0 0 0 0\\n\\\n",
    "Road.RL.1.RoadMarking.8.ID = '+str(obj_id+12)+' '+str(obj_id+2)+'\\n\\\n",
    "Road.RL.1.RoadMarking.8 = 0 0 0 1 0 1 0.15 0 1 0 0 2 4 1 1 0 \"\"\\n\\\n",
    "Road.RL.1.RoadMarking.8.Material.0 = 1.0,1.0,1.0 0 0 0 0 0 0 0 0 0 0 0\\n\\\n",
    "Road.MaxUsedObjId = '+str(obj_id+150)+'\\n\\\n",
    "Road.VhclStartPos = 0 0 0\\n\\\n",
    "Road.VhclRoute = Route_0\\n\\\n",
    "Road.RouteId = 0\\n\\\n",
    "Env.StartTime.Year = 2019\\n\\\n",
    "Env.StartTime.Month = 1\\n\\\n",
    "Env.StartTime.Day = 1\\n\\\n",
    "Env.StartTime.Hour = 12\\n\\\n",
    "Env.StartTime.Min = 0\\n\\\n",
    "Env.StartTime.Sec = 0\\n\\\n",
    "Env.StartTime.DeltaUTC = 0.0\\n\\\n",
    "Env.GNav.Active = 0\\n\\\n",
    "Env.Temperature = 20.0\\n\\\n",
    "Env.AirDensity = 1.205\\n\\\n",
    "Env.AirPressure = 1.013\\n\\\n",
    "Env.AirHumidity = 60\\n\\\n",
    "Env.SolarRadiation = 400.0\\n\\\n",
    "Env.RainRate = 0.0\\n\\\n",
    "Env.VisRangeInFog = 1000.0\\n\\\n",
    "Env.Wind.Kind = none\\n\\\n",
    "Env.Wind.Velocity = 0.0\\n\\\n",
    "Env.Wind.Angle = 0.0\\n\\\n",
    "Env.Sun.Position = geographicDefinition\\n\\\n",
    "Env.Sun.Azimuth = 180.0\\n\\\n",
    "Env.Sun.Elevation = 45.0\\n\\\n",
    "Env.Kind = Generic\\n\\\n",
    "Env.Temp.Offset_Elev = -0.0065\\n\\\n",
    "Env.Temp.Offset_sRoad.Amplify = 1.0\\n\\\n",
    "Env.Temp.Offset_sRoad.On = 0\\n\\\n",
    "Env.Temp.Offset_Time.Amplify = 1.0\\n\\\n",
    "Env.Temp.Offset_Time.On = 1\\n\\\n",
    "Env.Temp.Offset_Time:\\n\\\n",
    "    0.0 -2.0\\n\\\n",
    "    3.0 -2.5\\n\\\n",
    "    6.0 -2.7\\n\\\n",
    "    7.5 -2.7\\n\\\n",
    "    9.0 -2.5\\n\\\n",
    "    10.0 -2.3\\n\\\n",
    "    11.0 -1.6\\n\\\n",
    "    12.0 0.0\\n\\\n",
    "    13.0 1.4\\n\\\n",
    "    14.0 2.1\\n\\\n",
    "    15.5 2.5\\n\\\n",
    "    17.0 2.2\\n\\\n",
    "    18.0 1.7\\n\\\n",
    "    19.0 1.1\\n\\\n",
    "    20.0 0.2\\n\\\n",
    "    21.0 -0.6\\n\\\n",
    "    22.0 -1.1\\n\\\n",
    "    23.0 -1.6\\n\\\n",
    "    24.0 -2.0\\n\\\n",
    "Driver.ParamIdent = IPGDriver 5\\n\\\n",
    "Driver.Mode = std\\n\\\n",
    "Driver.Long.DrivMaxSpeed = 0\\n\\\n",
    "Driver.Long.CruisingSpeed = 150\\n\\\n",
    "Driver.CornerCutCoef = 0.5\\n\\\n",
    "Driver.ConsiderTraffic = 1\\n\\\n",
    "Driver.Traffic.TimeGapMin = 1.8\\n\\\n",
    "Driver.Traffic.TimeGapMax = 5.0\\n\\\n",
    "Driver.Traffic.DistMin = 6\\n\\\n",
    "Driver.Traffic.DistMax = 250\\n\\\n",
    "Driver.Traffic.EcoCoef = 0.75\\n\\\n",
    "Driver.Traffic.Overtake = 0\\n\\\n",
    "Driver.Traffic.Overtake_Rate = 1\\n\\\n",
    "Driver.Traffic.Overtake_dSpeedMin = 10\\n\\\n",
    "Driver.Long.dtAccBrake = 0.5\\n\\\n",
    "Driver.Long.axMax = 3.0\\n\\\n",
    "Driver.Long.axMin = -4.0\\n\\\n",
    "Driver.Long.ayMax = 4.0\\n\\\n",
    "Driver.Long.GGExp:\\n\\\n",
    "    50 1.0 1.0\\n\\\n",
    "Driver.Long.DevMax = 0.0\\n\\\n",
    "Driver.Long.tReact = 0.0\\n\\\n",
    "Driver.Long.TractionControl = 1\\n\\\n",
    "Driver.DecShift.UseBrakePark = 0\\n\\\n",
    "Driver.DecShift.tSwitchGear = 1.0\\n\\\n",
    "Driver.DecShift.nEngine.Limits:\\n\\\n",
    "    1500 4000\\n\\\n",
    "Driver.DecShift.nEngine.Shift:\\n\\\n",
    "    2000 3000\\n\\\n",
    "Driver.Lat.DevMax = 0.0\\n\\\n",
    "Driver.Lat.tReact = 0.0\\n\\\n",
    "Driver.Knowl.Long.tActionMin = 4\\n\\\n",
    "Driver.Knowl.Lat.StWhlAngleMax = 630\\n\\\n",
    "Driver.Knowl.Lat.StWhlAngleVelMax = 500\\n\\\n",
    "Driver.Knowl.Lat.StWhlAngleAccMax = 3000\\n\\\n",
    "Driver.Learn.VehicleLimits.TestRun =\\n\\\n",
    "Driver.Learn.VehicleLimits.Date = 0\\n\\\n",
    "Driver.Learn.ControllerDyn.TestRun =\\n\\\n",
    "Driver.Learn.ControllerDyn.Date = 0\\n\\\n",
    "Driver.Learn.MaxSpeed.TestRun =\\n\\\n",
    "Driver.Learn.MaxSpeed.Date = 0\\n\\\n",
    "Driver.Learn.Remember = 0\\n\\\n",
    "Driver.Learn.Friction = 1.0\\n\\\n",
    "Driver.Knowl.Long.tPreviewBra = 0.6\\n\\\n",
    "Driver.Knowl.Long.tPreviewAcc = 1.5\\n\\\n",
    "Driver.Knowl.Lat.tPreview = 0.8\\n\\\n",
    "Driver.Learn.NEng_S = 1\\n')\n",
    "\n",
    "\n",
    "    with open(TESTRUN_PATH, 'w') as f:\n",
    "        for item in content_lines:\n",
    "            f.write(\"%s\" % item)\n",
    "\n",
    "    print(f'Saved files to: {TESTRUN_PATH}')\n",
    "    \n",
    "    \n",
    "def outputvideo(scene_num):\n",
    "    import os\n",
    "    from shutil import copyfile\n",
    "\n",
    "    ## optional create video from camera data\n",
    "    #temp_path = LYFT_DATASET_ROOT+'/temp'\n",
    "    #if not os.path.exists(temp_path):\n",
    "    #    os.mkdir(temp_path)\n",
    "\n",
    "    img_list = []\n",
    "    scene = level5data.scene[scene_num]\n",
    "    scene_token = scene['token']\n",
    "    sample_token = scene['first_sample_token']\n",
    "    counter = 0\n",
    "    while sample_token is not '':\n",
    "        sample = level5data.get('sample', sample_token)\n",
    "        sample_data = level5data.get('sample_data', sample['data']['CAM_FRONT'])\n",
    "        img_list.append('file '+LYFT_DATASET_ROOT+sample_data['filename'])\n",
    "        #copyfile(LYFT_DATASET_ROOT+sample_data['filename'],temp_path+'/img_'+str(counter)+'.jpeg')\n",
    "        counter += 1\n",
    "        sample_token = sample['next']\n",
    "\n",
    "\n",
    "    img_list_path = LYFT_DATASET_ROOT+'/img_list.txt'\n",
    "    video_path = LYFT_DATASET_ROOT+'CarMaker/video-'+str(scene_num)+'.mp4'\n",
    "    with open(img_list_path, 'w') as f:\n",
    "        for item in img_list:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "\n",
    "    !ffmpeg -f concat -r 5 -safe 0 -i $img_list_path -r 5 -c:v libx264 -pix_fmt yuv420p $video_path\n",
    "\n",
    "    print('Video saved to: '+video_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load tracking data\n",
    "\n",
    "def loadDetectionDict(scene_num,config):\n",
    "    import json\n",
    "\n",
    "    #scene: index of scene\n",
    "    #config: small,middle,large\n",
    "\n",
    "    det_dict_path = '/home/itiv/Desktop/lyft-kaggle-dataset/train/CarMaker/gt_data/gt_data_scene-'+str(scene_num)+'-'+config+'.json'\n",
    "    det_dict = json.load(open(det_dict_path))\n",
    "    return det_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def ann_in_boundries(ego,ann,config,plot):\n",
    "    from pyquaternion import Quaternion\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    \n",
    "    \n",
    "    ###TODO!!!!!!!!!!\n",
    "    #detection boundries TODO filter ground truth out of detection boundary!!!\n",
    "    det_bound = [50,50,50,50] #dist to front,right,back,left\n",
    "    if model == 'middle':\n",
    "        det_bound = [100,70,70,70]\n",
    "    elif model == 'large':\n",
    "        det_bound  = [150,100,100,100]\n",
    "    elif model == 'smallv2':\n",
    "        det_bound  = [50,10,10,10]\n",
    "    elif model == 'middlev2':\n",
    "        det_bound  = [100,10,10,10]\n",
    "    elif model == 'largev2':\n",
    "        det_bound  = [150,10,10,10]\n",
    "\n",
    "    width = det_bound[1]+det_bound[3]\n",
    "    length = det_bound[0]+det_bound[2]\n",
    "    \n",
    "    center_dist = det_bound[0]-length/2 \n",
    "    \n",
    "    ego_yaw = Quaternion(ego['rotation']).yaw_pitch_roll[0]\n",
    "    ego_x_center = ego['translation'][0]\n",
    "    ego_y_center = ego['translation'][1]\n",
    "    \n",
    "    boundary_box_center_x = np.cos(ego_yaw)*center_dist+ego_x_center\n",
    "    boundary_box_center_y = np.sin(ego_yaw)*center_dist+ego_y_center\n",
    "    \n",
    "        \n",
    "\n",
    "    traffic_yaw = Quaternion(ann['rotation']).yaw_pitch_roll[0]\n",
    "    traffic_x_center = ann['translation'][0]\n",
    "    traffic_y_center = ann['translation'][1]\n",
    "    traffic_width = ann['size'][0]\n",
    "    traffic_height = ann['size'][1]\n",
    "    \n",
    "    ann_rect = []\n",
    "    ego_rect = []\n",
    "    \n",
    "    ann_rect.append([ann['translation'][0],ann['translation'][1],ann['size'][1],ann['size'][0],traffic_yaw])\n",
    "    ego_rect.append([boundary_box_center_x,boundary_box_center_y,length,width,ego_yaw])\n",
    "    \n",
    "    \n",
    "    ann_rect = np.array(ann_rect)\n",
    "    ego_rect = np.array(ego_rect)\n",
    "\n",
    "    ego_pos = rect_polygon(ego_x_center,ego_y_center,traffic_height,traffic_width,ego_yaw)\n",
    "    \n",
    "    \n",
    "    iou = intersection_over_union(ann_rect,ego_rect,plot)\n",
    "    \n",
    "    print(iou)\n",
    "    \n",
    "    return (iou[0,0]>0)\n",
    "\n",
    "\n",
    "\n",
    "def load_ground_truth_data(scene_num,config):\n",
    "    from pyquaternion import Quaternion\n",
    "    import operator\n",
    "    import math\n",
    "\n",
    "\n",
    "\n",
    "    scene = level5data.scene[scene_num]\n",
    "    scene_token = scene['token']\n",
    "\n",
    "    sample_token = scene['first_sample_token']\n",
    "    sample = level5data.get('sample', sample_token)\n",
    "    sample_data = level5data.get('sample_data', sample['data']['LIDAR_TOP'])\n",
    "    ego_pose = level5data.get('ego_pose',sample_data['ego_pose_token'])\n",
    "\n",
    "\n",
    "    trans_offset = ego_pose['translation']\n",
    "\n",
    "\n",
    "    gt_dict = {'samples':[]}\n",
    "    plot = True\n",
    "\n",
    "    for index in range(scene['nbr_samples']):\n",
    "        sample = level5data.get('sample', sample_token)\n",
    "\n",
    "        if(math.fmod(index,100) == 0):\n",
    "            plot = True\n",
    "            \n",
    "\n",
    "        sample_token = sample['next']\n",
    "        \n",
    "        #if sample_token == '':\n",
    "         #   print('Finished earlier at: '+str(index))\n",
    "        \n",
    "        sample_dict = {'token':sample_token,'timestamp':sample['timestamp'],'anns':[]}\n",
    "\n",
    "        sample_data = level5data.get('sample_data', sample['data']['LIDAR_TOP'])\n",
    "        ego_pose = level5data.get('ego_pose',sample_data['ego_pose_token'])\n",
    "        for ann_token in sample['anns']:\n",
    "            ann = level5data.get('sample_annotation', ann_token)\n",
    "            \n",
    "            if ann_in_boundries(ego_pose,ann,config,plot) and (ann['category_name'] == 'car' or ann['category_name'] == 'bus' or ann['category_name'] == 'construction_vehicle' or ann['category_name'] == 'motorcycle' or ann['category_name'] == 'truck' or ann['category_name'] == 'trailer'):     \n",
    "                yaw = Quaternion(ann['rotation']).yaw_pitch_roll[0]\n",
    "                ann_dict = {'instance':ann['instance_token'],'translation':list(map(operator.sub,ann['translation'],trans_offset)),'size':ann['size'],'rotation':yaw}\n",
    "                sample_dict['anns'].append(ann_dict)\n",
    "\n",
    "        plot = False\n",
    "        gt_dict['samples'].append(sample_dict)\n",
    "        \n",
    "        \n",
    "    return gt_dict\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://arxiv.org/pdf/1603.00831.pdf\n",
    "#\n",
    "#\n",
    "#Multiple Object Tracking Accuracy\n",
    "# ratio of misses in the sequence, computed over the total number of objects present in all frames\n",
    "# ratio of false positives (bounding box with iou < T)\n",
    "# ratio of mismatches (id change)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def rect_polygon(x, y, width, height, angle):\n",
    "    from shapely.geometry import Polygon\n",
    "    from shapely.affinity import rotate, translate\n",
    "    import math\n",
    "\n",
    "    \"\"\"Return a shapely Polygon describing the rectangle with centre at\n",
    "    (x, y) and the given width and height, rotated by angle quarter-turns.\n",
    "\n",
    "    \"\"\"\n",
    "    w = width / 2\n",
    "    h = height / 2\n",
    "    p = Polygon([(-w, -h), (w, -h), (w, h), (-w, h)])\n",
    "    return translate(rotate(p, math.degrees(angle)), x, y)\n",
    "\n",
    "\n",
    "def intersection_over_union(rects_a, rects_b,plot=False):\n",
    "    import rtree.index\n",
    "    from shapely.affinity import rotate, translate\n",
    "    from shapely.geometry import Polygon\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import math\n",
    "\n",
    "    \"\"\"Calculate the intersection-over-union for every pair of rectangles\n",
    "    in the two arrays.\n",
    "\n",
    "    Arguments:\n",
    "    rects_a: array_like, shape=(M, 5)\n",
    "    rects_b: array_like, shape=(N, 5)\n",
    "        Rotated rectangles, represented as (centre x, centre y, width,\n",
    "        height, rotation in quarter-turns).\n",
    "\n",
    "    Returns:\n",
    "    iou: array, shape=(M, N)\n",
    "        Array whose element i, j is the intersection-over-union\n",
    "        measure for rects_a[i] and rects_b[j].\n",
    "\n",
    "    \"\"\"\n",
    "    m = len(rects_a)\n",
    "    n = len(rects_b)\n",
    "    if m > n:\n",
    "        # More memory-efficient to compute it the other way round and\n",
    "        # transpose.\n",
    "        return intersection_over_union(rects_b, rects_a,plot).T\n",
    "\n",
    "    # Convert rects_a to shapely Polygon objects.\n",
    "    polys_a = [rect_polygon(*r) for r in rects_a]\n",
    "\n",
    "    # Build a spatial index for rects_a.\n",
    "    index_a = rtree.index.Index()\n",
    "    for i, a in enumerate(polys_a):\n",
    "        index_a.insert(i, a.bounds)\n",
    "    \n",
    "    #print((index_a))\n",
    "        \n",
    "    # Find candidate intersections using the spatial index.\n",
    "    iou = np.zeros((m, n))\n",
    "    for j, rect_b in enumerate(rects_b):\n",
    "        b = rect_polygon(*rect_b)\n",
    "        if plot:\n",
    "            x,y = b.exterior.xy\n",
    "            if x[0] < 500 and y[0] < 500:\n",
    "                plt.plot(x,y,'g-')\n",
    "        for i in range(len(polys_a)):#index_a.intersection(b.bounds):\n",
    "            a = polys_a[i]\n",
    "            if(j == 1 and plot):\n",
    "                x,y = a.exterior.xy\n",
    "                if x[0] < 500 and y[0] < 500:\n",
    "                    plt.plot(x,y,'r-')\n",
    "        \n",
    "            intersection_area = a.intersection(b).area\n",
    "            if intersection_area:\n",
    "                iou[i, j] = intersection_area / a.union(b).area\n",
    "    \n",
    "    if plot:\n",
    "        plt.show()\n",
    "        print(iou)\n",
    "    return iou\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_iou_for_scene(gt_dict,det_dict,id_counter_init):\n",
    "    \n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    iou_arr = []\n",
    "    gt_obj_id = []\n",
    "    det_obj_id = []\n",
    "\n",
    "    global id_counter\n",
    "    id_counter = id_counter_init\n",
    "    global id_conversion_dict\n",
    "    id_conversion_dict = {}\n",
    "\n",
    "\n",
    "    def instance_token_to_id(token):\n",
    "        global id_counter\n",
    "        global id_conversion_dict\n",
    "        if not token in id_conversion_dict:\n",
    "            id_counter += 1\n",
    "            id_conversion_dict.update( {token : id_counter} )\n",
    "\n",
    "        return id_conversion_dict[token]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for sample_index in range(len(gt_dict['samples'])):\n",
    "\n",
    "        gt_rects = []\n",
    "        gt_ids = []\n",
    "        for anns in gt_dict['samples'][sample_index]['anns']:\n",
    "            rect = [anns['translation'][0],anns['translation'][1],anns['size'][1],anns['size'][0],anns['rotation']]\n",
    "            gt_rects.append(rect)\n",
    "            gt_ids.append(instance_token_to_id(anns['instance']))\n",
    "\n",
    "        det_rects = []\n",
    "        det_ids = []\n",
    "        for anns in det_dict['samples'][sample_index]['anns']:\n",
    "            rect = [anns['translation'][0],anns['translation'][1],anns['size'][1],anns['size'][0],anns['rotation']]\n",
    "            det_rects.append(rect)\n",
    "            det_ids.append(anns['instance'])\n",
    "\n",
    "\n",
    "        gt_obj_id.append(gt_ids)\n",
    "        det_obj_id.append(det_ids)\n",
    "\n",
    "\n",
    "        gt_rects = np.array(gt_rects)\n",
    "        det_rects = np.array(det_rects)\n",
    "\n",
    "\n",
    "        iou_arr.append(intersection_over_union(gt_rects,det_rects,id_counter_init == 0))\n",
    "\n",
    "        ### returns \n",
    "        #  det det det det det\n",
    "        #gt\n",
    "        #gt\n",
    "        #gt\n",
    "        #gt\n",
    "        #gt\n",
    "        #print(intersection_over_union(gt_rects,det_rects,False))\n",
    "        #print(len(gt_rects))\n",
    "        #print(len(det_rects))\n",
    "\n",
    "\n",
    "\n",
    "    return iou_arr,gt_obj_id,det_obj_id,id_counter\n",
    "\n",
    "\n",
    "    '''\n",
    "\n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sum of gt\n",
    "## sum of misses\n",
    "## sum of false positive\n",
    "## sum of mismatch\n",
    "\n",
    "## dist = 1 - iou\n",
    "\n",
    "IOU_TRESHOLD = 0.5\n",
    "\n",
    "\n",
    "def get_MOTA_Acc(iou_arr,gt_obj_id,det_obj_id):\n",
    "    import motmetrics as mm\n",
    "    import numpy as np\n",
    "\n",
    "    # Create an accumulator that will be updated during each frame\n",
    "    acc = mm.MOTAccumulator(auto_id=True)\n",
    "\n",
    "    for index,iou in enumerate(iou_arr):\n",
    "\n",
    "        \n",
    "        #dist = np.subtract(np.ones(iou.shape),iou)\n",
    "        dist = iou\n",
    "        \n",
    "        \n",
    "\n",
    "        for i, r in enumerate(dist):\n",
    "            for j, c in enumerate(r):\n",
    "                if dist[i,j] > (1-IOU_TRESHOLD):\n",
    "                    dist[i,j] = np.nan\n",
    "\n",
    "        # Call update once for per frame. For now, assume distances between\n",
    "        # frame objects / hypotheses are given.\n",
    "        acc.update(\n",
    "            gt_obj_id[index],                     # Ground truth objects in this frame\n",
    "            det_obj_id[index],                  # Detector hypotheses in this frame\n",
    "            dist.tolist()\n",
    "        )\n",
    "    return acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 category,\n",
      "18 attribute,\n",
      "4 visibility,\n",
      "15991 instance,\n",
      "8 sensor,\n",
      "128 calibrated_sensor,\n",
      "149072 ego_pose,\n",
      "148 log,\n",
      "148 scene,\n",
      "18634 sample,\n",
      "149072 sample_data,\n",
      "539765 sample_annotation,\n",
      "1 map,\n",
      "Done loading in 6.4 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 1.9 seconds.\n",
      "======\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 4,\n",
       " 6,\n",
       " 12,\n",
       " 25,\n",
       " 30,\n",
       " 32,\n",
       " 41,\n",
       " 45,\n",
       " 46,\n",
       " 48,\n",
       " 61,\n",
       " 70,\n",
       " 76,\n",
       " 79,\n",
       " 80,\n",
       " 83,\n",
       " 84,\n",
       " 87,\n",
       " 104,\n",
       " 108,\n",
       " 114,\n",
       " 120,\n",
       " 130,\n",
       " 134,\n",
       " 141,\n",
       " 142,\n",
       " 146,\n",
       " 153,\n",
       " 174,\n",
       " 175,\n",
       " 176]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### get test scenes!\n",
    "# Load the SDK\n",
    "%matplotlib inline\n",
    "from lyft_dataset_sdk.lyftdataset import LyftDataset\n",
    "\n",
    "# Load the dataset\n",
    "# Adjust the dataroot parameter below to point to your local dataset path.\n",
    "# The correct dataset path contains at least the following four folders (or similar): images, lidar, maps, v1.0.1-train\n",
    "level5datalyft = LyftDataset(data_path='/home/itiv/Desktop/lyft-dataset/', json_path='/home/itiv/Desktop/lyft-dataset/'+'/v1.02-train', verbose=True)\n",
    "\n",
    "## extract test scenes...\n",
    "counter = 0\n",
    "test_scene_index = []\n",
    "for kaggleindex,kagglescene in enumerate(level5data.scene):\n",
    "    for index,lyftscene in enumerate(level5datalyft.scene):\n",
    "        if kagglescene['name'] == lyftscene['name']:\n",
    "            counter += 1\n",
    "            break\n",
    "        elif index == len(level5datalyft.scene)-1:\n",
    "            #not found:\n",
    "            counter += 1\n",
    "            test_scene_index.append(kaggleindex)\n",
    "\n",
    "\n",
    "test_scene_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "index = 3\n",
    "\n",
    "scene = level5data.scene[test_scene_index[index]]\n",
    "scene_token = scene['token']\n",
    "\n",
    "print(test_scene_index[index])\n",
    "\n",
    "level5data.render_scene(scene_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########eval#############\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_scene_index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-31b00454f40d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mid_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0maccs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mscene_num\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_scene_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0;31m#try:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m#create_pkl(scene_num)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_scene_index' is not defined"
     ]
    }
   ],
   "source": [
    "## run this\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "import motmetrics as mm\n",
    "import numpy as np  \n",
    "from datetime import datetime\n",
    "\n",
    "#models = ['middle','middle','large','large','large']\n",
    "#nets = ['/model_middle_range/voxelnet-550000.tckpt','/model_middle_range/voxelnet-890670.tckpt','/model_large_range/voxelnet-250000.tckpt','/model_large_range/voxelnet-500000.tckpt','/model_large_range/voxelnet-890670.tckpt']\n",
    "\n",
    "#models = ['large','large','large','small','small']\n",
    "#nets = ['/model_large_range/voxelnet-250000.tckpt','/model_large_range/voxelnet-500000.tckpt','/model_large_range/voxelnet-890670.tckpt','/model_small_range/voxelnet-259585.tckpt','/model_small_range/voxelnet-121598.tckpt']\n",
    "\n",
    "models = ['small']\n",
    "nets = ['/model_small_range/voxelnet-259585.tckpt']\n",
    "\n",
    "\n",
    "skip_scenes = [0,46]\n",
    "\n",
    "eval_scenes = [30]\n",
    "\n",
    "##create pkl    \n",
    "#for index,model in enumerate(models):\n",
    "#    for scene_num in test_scene_index:\n",
    "#   \n",
    "\n",
    "    #outputvideo(scene_num)\n",
    "\n",
    "print('###########eval#############')        \n",
    "for index,model in enumerate(models):\n",
    "    iou_arr = []\n",
    "    gt_ids = []\n",
    "    det_ids = []\n",
    "    id_counter = 0\n",
    "    accs = []\n",
    "    for scene_num in test_scene_index:\n",
    "        \n",
    "        if scene_num in []:\n",
    "        \n",
    "        \n",
    "    #try:\n",
    "        #create_pkl(scene_num)\n",
    "        print(scene_num)\n",
    "        set_config_file(scene_num,model)\n",
    "        track(scene_num,model,nets[index])\n",
    "\n",
    "        det_dict = loadDetectionDict(scene_num,model)\n",
    "        gt_dict = load_ground_truth_data(scene_num,model)\n",
    "        scene_iou_arr,scene_gt_ids,scene_det_ids,id_counter_val = get_iou_for_scene(gt_dict,det_dict,id_counter)\n",
    "        id_counter = id_counter_val\n",
    "        #accs.append(get_MOTA_Acc(scene_iou_arr,scene_gt_ids,scene_det_ids))\n",
    "        det_ids.extend(scene_det_ids)\n",
    "        gt_ids.extend(scene_gt_ids)\n",
    "        iou_arr.extend(scene_iou_arr)\n",
    "        print(f'finished analyzing scene: {scene_num}')\n",
    "    #except:\n",
    "        print('#################### ERROR at '+str(scene_num)+' ########################')\n",
    "\n",
    "        \n",
    "    acc = get_MOTA_Acc(iou_arr,gt_ids,det_ids)\n",
    "\n",
    "    print('creating metric')\n",
    "    \n",
    "    mh = mm.metrics.create()\n",
    "    summary = mh.compute(acc, metrics=mm.metrics.motchallenge_metrics, name=model)\n",
    "    print(summary)\n",
    "    \n",
    "    now = datetime.now() # current date and time\n",
    "    date_time = now.strftime(\"%m/%d/%Y, %H:%M:%S\")\n",
    "    \n",
    "    with open('EvaluationResults.txt', \"a\") as resultFile:\n",
    "        resultFile.write(\"\\n #### \"+str(date_time)+' ### \\n')\n",
    "        resultFile.write(\"Net: \"+nets[index]+' \\n')\n",
    "\n",
    "    \n",
    "    summary.to_csv('EvaluationResults.txt', sep='\\t', mode='a',index=True)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########eval#############\n",
      "30\n",
      "Paths set in config file\n",
      "Detecting...this might take several minutes...\n",
      "/home/itiv/anaconda3/envs/lyft_detector/lib/python3.6/site-packages/numba/cuda/envvars.py:17: NumbaWarning: \n",
      "Environment variables with the 'NUMBAPRO' prefix are deprecated and consequently ignored, found use of NUMBAPRO_NVVM=/usr/local/cuda/nvvm/lib64/libnvvm.so.\n",
      "\n",
      "For more information about alternatives visit: ('http://numba.pydata.org/numba-doc/latest/cuda/overview.html', '#cudatoolkit-lookup')\n",
      "  warnings.warn(errors.NumbaWarning(msg))\n",
      "/home/itiv/anaconda3/envs/lyft_detector/lib/python3.6/site-packages/numba/cuda/envvars.py:17: NumbaWarning: \n",
      "Environment variables with the 'NUMBAPRO' prefix are deprecated and consequently ignored, found use of NUMBAPRO_LIBDEVICE=/usr/local/cuda/nvvm/libdevice.\n",
      "\n",
      "For more information about alternatives visit: ('http://numba.pydata.org/numba-doc/latest/cuda/overview.html', '#cudatoolkit-lookup')\n",
      "  warnings.warn(errors.NumbaWarning(msg))\n",
      "/home/itiv/anaconda3/envs/lyft_detector/lib/python3.6/site-packages/numba/cuda/envvars.py:17: NumbaWarning: \n",
      "Environment variables with the 'NUMBAPRO' prefix are deprecated and consequently ignored, found use of NUMBAPRO_CUDA_DRIVER=/usr/lib/x86_64-linux-gnu/libcuda.so.\n",
      "\n",
      "For more information about alternatives visit: ('http://numba.pydata.org/numba-doc/latest/cuda/overview.html', '#cudatoolkit-lookup')\n",
      "  warnings.warn(errors.NumbaWarning(msg))\n",
      "Traceback (most recent call last):\n",
      "  File \"./Lyft-Detector/second.pytorch/second/detect.py\", line 284, in <module>\n",
      "    fire.Fire()\n",
      "  File \"/home/itiv/anaconda3/envs/lyft_detector/lib/python3.6/site-packages/fire/core.py\", line 138, in Fire\n",
      "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
      "  File \"/home/itiv/anaconda3/envs/lyft_detector/lib/python3.6/site-packages/fire/core.py\", line 471, in _Fire\n",
      "    target=component.__name__)\n",
      "  File \"/home/itiv/anaconda3/envs/lyft_detector/lib/python3.6/site-packages/fire/core.py\", line 675, in _CallAndUpdateTrace\n",
      "    component = fn(*varargs, **kwargs)\n",
      "  File \"./Lyft-Detector/second.pytorch/second/detect.py\", line 62, in detect\n",
      "    net.load_state_dict(torch.load(ckpt_path))\n",
      "  File \"/home/itiv/anaconda3/envs/lyft_detector/lib/python3.6/site-packages/torch/serialization.py\", line 525, in load\n",
      "    with _open_file_like(f, 'rb') as opened_file:\n",
      "  File \"/home/itiv/anaconda3/envs/lyft_detector/lib/python3.6/site-packages/torch/serialization.py\", line 212, in _open_file_like\n",
      "    return _open_file(name_or_buffer, mode)\n",
      "  File \"/home/itiv/anaconda3/envs/lyft_detector/lib/python3.6/site-packages/torch/serialization.py\", line 193, in __init__\n",
      "    super(_open_file, self).__init__(open(name, mode))\n",
      "FileNotFoundError: [Errno 2] No such file or directory: './Lyft-Detector/second.pytorch/second/model//model_small_range/voxelnet-259585.tckpt'\n",
      "Tracking...this might take several minutes...\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"./Lyft-Tracker/mahalanobis_3d_multi_object_tracking/main.py\", line 4, in <module>\n",
      "    import os.path, copy, numpy as np, time, sys\n",
      "  File \"/home/itiv/anaconda3/envs/lyft_detector/lib/python3.6/site-packages/numpy/__init__.py\", line 161, in <module>\n",
      "    from . import lib\n",
      "  File \"/home/itiv/anaconda3/envs/lyft_detector/lib/python3.6/site-packages/numpy/lib/__init__.py\", line 13, in <module>\n",
      "    from .shape_base import *\n",
      "  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 951, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 894, in _find_spec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1157, in find_spec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1129, in _get_spec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1241, in find_spec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 82, in _path_stat\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-6603a037361f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscene_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mset_config_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscene_num\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mtrack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscene_num\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mdet_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloadDetectionDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscene_num\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-f74ed6bdb8f4>\u001b[0m in \u001b[0;36mtrack\u001b[0;34m(scene_num, model, net)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0mcounter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0msample_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'next'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_token\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "    \n",
    "import motmetrics as mm\n",
    "import numpy as np  \n",
    "from datetime import datetime\n",
    "\n",
    "#models = ['middle','middle','large','large','large']\n",
    "#nets = ['/model_middle_range/voxelnet-550000.tckpt','/model_middle_range/voxelnet-890670.tckpt','/model_large_range/voxelnet-250000.tckpt','/model_large_range/voxelnet-500000.tckpt','/model_large_range/voxelnet-890670.tckpt']\n",
    "\n",
    "#models = ['large','large','large','small','small']\n",
    "#nets = ['/model_large_range/voxelnet-250000.tckpt','/model_large_range/voxelnet-500000.tckpt','/model_large_range/voxelnet-890670.tckpt','/model_small_range/voxelnet-259585.tckpt','/model_small_range/voxelnet-121598.tckpt']\n",
    "\n",
    "models = ['small']\n",
    "nets = ['/model_small_range/voxelnet-259585.tckpt']\n",
    "\n",
    "\n",
    "skip_scenes = [0,46]\n",
    "\n",
    "eval_scenes = [30]\n",
    "\n",
    "##create pkl    \n",
    "#for index,model in enumerate(models):\n",
    "#    for scene_num in test_scene_index:\n",
    "#   \n",
    "\n",
    "    #outputvideo(scene_num)\n",
    "\n",
    "print('###########eval#############')        \n",
    "for index,model in enumerate(models):\n",
    "    iou_arr = []\n",
    "    gt_ids = []\n",
    "    det_ids = []\n",
    "    id_counter = 0\n",
    "    accs = []\n",
    "    for scene_num in eval_scenes:\n",
    "        #create_pkl(scene_num)\n",
    "        print(scene_num)\n",
    "        set_config_file(scene_num,model)\n",
    "        track(scene_num,model,nets[index])\n",
    "\n",
    "        det_dict = loadDetectionDict(scene_num,model)\n",
    "        gt_dict = load_ground_truth_data(scene_num,model)\n",
    "        scene_iou_arr,scene_gt_ids,scene_det_ids,id_counter_val = get_iou_for_scene(gt_dict,det_dict,id_counter)\n",
    "        id_counter = id_counter_val\n",
    "        #accs.append(get_MOTA_Acc(scene_iou_arr,scene_gt_ids,scene_det_ids))\n",
    "        det_ids.extend(scene_det_ids)\n",
    "        gt_ids.extend(scene_gt_ids)\n",
    "        iou_arr.extend(scene_iou_arr)\n",
    "        print(f'finished analyzing scene: {scene_num}')\n",
    "        print('#################### ERROR at '+str(scene_num)+' ########################')\n",
    "\n",
    "        \n",
    "    acc = get_MOTA_Acc(iou_arr,gt_ids,det_ids)\n",
    "\n",
    "    print('creating metric')\n",
    "    \n",
    "    mh = mm.metrics.create()\n",
    "    summary = mh.compute(acc, metrics=mm.metrics.motchallenge_metrics, name=model)\n",
    "    print(summary)\n",
    "    \n",
    "    now = datetime.now() # current date and time\n",
    "    date_time = now.strftime(\"%m/%d/%Y, %H:%M:%S\")\n",
    "    \n",
    "    with open('EvaluationResults.txt', \"a\") as resultFile:\n",
    "        resultFile.write(\"\\n #### \"+str(date_time)+' ### single scene analysis \\n')\n",
    "        resultFile.write(\"Net: \"+nets[index]+' \\n')\n",
    "\n",
    "    \n",
    "    summary.to_csv('EvaluationResults.txt', sep='\\t', mode='a',index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paths set in config file\n",
      "/home/itiv/anaconda3/envs/lyft_detector/lib/python3.6/site-packages/numba/cuda/envvars.py:17: NumbaWarning: \n",
      "Environment variables with the 'NUMBAPRO' prefix are deprecated and consequently ignored, found use of NUMBAPRO_NVVM=/usr/local/cuda/nvvm/lib64/libnvvm.so.\n",
      "\n",
      "For more information about alternatives visit: ('http://numba.pydata.org/numba-doc/latest/cuda/overview.html', '#cudatoolkit-lookup')\n",
      "  warnings.warn(errors.NumbaWarning(msg))\n",
      "/home/itiv/anaconda3/envs/lyft_detector/lib/python3.6/site-packages/numba/cuda/envvars.py:17: NumbaWarning: \n",
      "Environment variables with the 'NUMBAPRO' prefix are deprecated and consequently ignored, found use of NUMBAPRO_LIBDEVICE=/usr/local/cuda/nvvm/libdevice.\n",
      "\n",
      "For more information about alternatives visit: ('http://numba.pydata.org/numba-doc/latest/cuda/overview.html', '#cudatoolkit-lookup')\n",
      "  warnings.warn(errors.NumbaWarning(msg))\n",
      "/home/itiv/anaconda3/envs/lyft_detector/lib/python3.6/site-packages/numba/cuda/envvars.py:17: NumbaWarning: \n",
      "Environment variables with the 'NUMBAPRO' prefix are deprecated and consequently ignored, found use of NUMBAPRO_CUDA_DRIVER=/usr/lib/x86_64-linux-gnu/libcuda.so.\n",
      "\n",
      "For more information about alternatives visit: ('http://numba.pydata.org/numba-doc/latest/cuda/overview.html', '#cudatoolkit-lookup')\n",
      "  warnings.warn(errors.NumbaWarning(msg))\n",
      "num parameters: 66\n",
      "False _amp_stash\n",
      "load 15358 infos database infos\n",
      "load 1 metadata database infos\n",
      "After filter database:\n",
      "load 15358 infos database infos\n",
      "load 1 metadata database infos\n",
      "model {\n",
      "  second {\n",
      "    network_class_name: \"VoxelNet\"\n",
      "    voxel_generator {\n",
      "      voxel_size: 0.25\n",
      "      voxel_size: 0.25\n",
      "      voxel_size: 20.0\n",
      "      point_cloud_range: -50.0\n",
      "      point_cloud_range: -10.0\n",
      "      point_cloud_range: -10.0\n",
      "      point_cloud_range: 10.0\n",
      "      point_cloud_range: 10.0\n",
      "      point_cloud_range: 10.0\n",
      "      max_number_of_points_per_voxel: 60\n",
      "    }\n",
      "    voxel_feature_extractor {\n",
      "      module_class_name: \"PillarFeatureNet\"\n",
      "      num_filters: 64\n",
      "      num_input_features: 4\n",
      "    }\n",
      "    middle_feature_extractor {\n",
      "      module_class_name: \"PointPillarsScatter\"\n",
      "      num_input_features: 64\n",
      "      downsample_factor: 1\n",
      "    }\n",
      "    rpn {\n",
      "      module_class_name: \"RPNV2\"\n",
      "      layer_nums: 3\n",
      "      layer_nums: 5\n",
      "      layer_nums: 5\n",
      "      layer_strides: 2\n",
      "      layer_strides: 2\n",
      "      layer_strides: 2\n",
      "      num_filters: 64\n",
      "      num_filters: 128\n",
      "      num_filters: 256\n",
      "      upsample_strides: 0.25\n",
      "      upsample_strides: 0.5\n",
      "      upsample_strides: 1.0\n",
      "      num_upsample_filters: 128\n",
      "      num_upsample_filters: 128\n",
      "      num_upsample_filters: 128\n",
      "      num_groups: 32\n",
      "      num_input_features: 64\n",
      "    }\n",
      "    num_point_features: 4\n",
      "    use_sigmoid_score: true\n",
      "    loss {\n",
      "      localization_loss {\n",
      "        weighted_smooth_l1 {\n",
      "          sigma: 3.0\n",
      "          code_weight: 1.0\n",
      "          code_weight: 1.0\n",
      "          code_weight: 1.0\n",
      "          code_weight: 1.0\n",
      "          code_weight: 1.0\n",
      "          code_weight: 1.0\n",
      "          code_weight: 1.0\n",
      "        }\n",
      "      }\n",
      "      classification_loss {\n",
      "        weighted_sigmoid_focal {\n",
      "          anchorwise_output: true\n",
      "          gamma: 2.0\n",
      "          alpha: 0.25\n",
      "        }\n",
      "      }\n",
      "      classification_weight: 1.0\n",
      "      localization_weight: 2.0\n",
      "    }\n",
      "    encode_rad_error_by_sin: true\n",
      "    encode_background_as_zeros: true\n",
      "    use_direction_classifier: true\n",
      "    direction_loss_weight: 0.2\n",
      "    pos_class_weight: 1.0\n",
      "    neg_class_weight: 1.0\n",
      "    loss_norm_type: NormByNumPositives\n",
      "    box_coder {\n",
      "      ground_box3d_coder {\n",
      "      }\n",
      "    }\n",
      "    target_assigner {\n",
      "      class_settings {\n",
      "        anchor_generator_range {\n",
      "          sizes: 1.9301772\n",
      "          sizes: 4.7671814\n",
      "          sizes: 1.7227076\n",
      "          anchor_ranges: -49.599998\n",
      "          anchor_ranges: -9.5999985\n",
      "          anchor_ranges: -1.0700001\n",
      "          anchor_ranges: 9.5999985\n",
      "          anchor_ranges: 9.5999985\n",
      "          anchor_ranges: -1.0700001\n",
      "          rotations: 0.0\n",
      "          rotations: 1.5700001\n",
      "        }\n",
      "        region_similarity_calculator {\n",
      "          nearest_iou_similarity {\n",
      "          }\n",
      "        }\n",
      "        nms_pre_max_size: 1000\n",
      "        nms_post_max_size: 300\n",
      "        nms_score_threshold: 0.050000001\n",
      "        nms_iou_threshold: 0.30000001\n",
      "        matched_threshold: 0.40000001\n",
      "        unmatched_threshold: 0.30000001\n",
      "        class_name: \"car\"\n",
      "      }\n",
      "      class_settings {\n",
      "        anchor_generator_range {\n",
      "          sizes: 0.63058913\n",
      "          sizes: 1.7645216\n",
      "          sizes: 1.4419219\n",
      "          anchor_ranges: -49.599998\n",
      "          anchor_ranges: -9.5999985\n",
      "          anchor_ranges: -1.0700001\n",
      "          anchor_ranges: 9.5999985\n",
      "          anchor_ranges: 9.5999985\n",
      "          anchor_ranges: -1.0700001\n",
      "          rotations: 0.0\n",
      "          rotations: 1.5700001\n",
      "        }\n",
      "        region_similarity_calculator {\n",
      "          nearest_iou_similarity {\n",
      "          }\n",
      "        }\n",
      "        nms_pre_max_size: 1000\n",
      "        nms_post_max_size: 300\n",
      "        nms_score_threshold: 0.050000001\n",
      "        nms_iou_threshold: 0.30000001\n",
      "        matched_threshold: 0.2\n",
      "        unmatched_threshold: 0.15000001\n",
      "        class_name: \"bicycle\"\n",
      "      }\n",
      "      class_settings {\n",
      "        anchor_generator_range {\n",
      "          sizes: 0.36058912\n",
      "          sizes: 0.73452163\n",
      "          sizes: 0.5192197\n",
      "          anchor_ranges: -49.599998\n",
      "          anchor_ranges: -9.5999985\n",
      "          anchor_ranges: -1.79\n",
      "          anchor_ranges: 9.5999985\n",
      "          anchor_ranges: 9.5999985\n",
      "          anchor_ranges: -1.79\n",
      "          rotations: 0.0\n",
      "          rotations: 1.5700001\n",
      "        }\n",
      "        region_similarity_calculator {\n",
      "          nearest_iou_similarity {\n",
      "          }\n",
      "        }\n",
      "        nms_pre_max_size: 1000\n",
      "        nms_post_max_size: 300\n",
      "        nms_score_threshold: 0.050000001\n",
      "        nms_iou_threshold: 0.30000001\n",
      "        matched_threshold: 0.2\n",
      "        unmatched_threshold: 0.15000001\n",
      "        class_name: \"animal\"\n",
      "      }\n",
      "      class_settings {\n",
      "        anchor_generator_range {\n",
      "          sizes: 2.960469\n",
      "          sizes: 12.348599\n",
      "          sizes: 3.4403098\n",
      "          anchor_ranges: -49.599998\n",
      "          anchor_ranges: -9.5999985\n",
      "          anchor_ranges: -0.34999999\n",
      "          anchor_ranges: 9.5999985\n",
      "          anchor_ranges: 9.5999985\n",
      "          anchor_ranges: -0.34999999\n",
      "          rotations: 0.0\n",
      "          rotations: 1.5700001\n",
      "        }\n",
      "        region_similarity_calculator {\n",
      "          nearest_iou_similarity {\n",
      "          }\n",
      "        }\n",
      "        nms_pre_max_size: 1000\n",
      "        nms_post_max_size: 300\n",
      "        nms_score_threshold: 0.050000001\n",
      "        nms_iou_threshold: 0.30000001\n",
      "        matched_threshold: 0.30000001\n",
      "        unmatched_threshold: 0.15000001\n",
      "        class_name: \"bus\"\n",
      "      }\n",
      "      class_settings {\n",
      "        anchor_generator_range {\n",
      "          sizes: 2.450469\n",
      "          sizes: 6.5285993\n",
      "          sizes: 2.3903098\n",
      "          anchor_ranges: -49.599998\n",
      "          anchor_ranges: -9.5999985\n",
      "          anchor_ranges: -0.88\n",
      "          anchor_ranges: 9.5999985\n",
      "          anchor_ranges: 9.5999985\n",
      "          anchor_ranges: -0.88\n",
      "          rotations: 0.0\n",
      "          rotations: 1.5700001\n",
      "        }\n",
      "        region_similarity_calculator {\n",
      "          nearest_iou_similarity {\n",
      "          }\n",
      "        }\n",
      "        nms_pre_max_size: 1000\n",
      "        nms_post_max_size: 300\n",
      "        nms_score_threshold: 0.050000001\n",
      "        nms_iou_threshold: 0.30000001\n",
      "        matched_threshold: 0.5\n",
      "        unmatched_threshold: 0.34999999\n",
      "        class_name: \"emergency_vehicle\"\n",
      "      }\n",
      "      class_settings {\n",
      "        anchor_generator_range {\n",
      "          sizes: 2.7905047\n",
      "          sizes: 8.2035294\n",
      "          sizes: 3.2331243\n",
      "          anchor_ranges: -49.599998\n",
      "          anchor_ranges: -9.5999985\n",
      "          anchor_ranges: -0.62\n",
      "          anchor_ranges: 9.5999985\n",
      "          anchor_ranges: 9.5999985\n",
      "          anchor_ranges: -0.62\n",
      "          rotations: 0.0\n",
      "          rotations: 1.5700001\n",
      "        }\n",
      "        region_similarity_calculator {\n",
      "          nearest_iou_similarity {\n",
      "          }\n",
      "        }\n",
      "        nms_pre_max_size: 1000\n",
      "        nms_post_max_size: 300\n",
      "        nms_score_threshold: 0.050000001\n",
      "        nms_iou_threshold: 0.30000001\n",
      "        matched_threshold: 0.5\n",
      "        unmatched_threshold: 0.30000001\n",
      "        class_name: \"other_vehicle\"\n",
      "      }\n",
      "      class_settings {\n",
      "        anchor_generator_range {\n",
      "          sizes: 0.96279478\n",
      "          sizes: 2.3597379\n",
      "          sizes: 1.5940304\n",
      "          anchor_ranges: -49.599998\n",
      "          anchor_ranges: -9.5999985\n",
      "          anchor_ranges: -1.3200001\n",
      "          anchor_ranges: 9.5999985\n",
      "          anchor_ranges: 9.5999985\n",
      "          anchor_ranges: -1.3200001\n",
      "          rotations: 0.0\n",
      "          rotations: 1.5700001\n",
      "        }\n",
      "        region_similarity_calculator {\n",
      "          nearest_iou_similarity {\n",
      "          }\n",
      "        }\n",
      "        nms_pre_max_size: 1000\n",
      "        nms_post_max_size: 300\n",
      "        nms_score_threshold: 0.050000001\n",
      "        nms_iou_threshold: 0.30000001\n",
      "        matched_threshold: 0.2\n",
      "        unmatched_threshold: 0.15000001\n",
      "        class_name: \"motorcycle\"\n",
      "      }\n",
      "      class_settings {\n",
      "        anchor_generator_range {\n",
      "          sizes: 0.77344888\n",
      "          sizes: 0.81564373\n",
      "          sizes: 1.7874807\n",
      "          anchor_ranges: -49.599998\n",
      "          anchor_ranges: -9.5999985\n",
      "          anchor_ranges: -0.91000003\n",
      "          anchor_ranges: 9.5999985\n",
      "          anchor_ranges: 9.5999985\n",
      "          anchor_ranges: -0.91000003\n",
      "          rotations: 0.0\n",
      "        }\n",
      "        region_similarity_calculator {\n",
      "          nearest_iou_similarity {\n",
      "          }\n",
      "        }\n",
      "        nms_pre_max_size: 1000\n",
      "        nms_post_max_size: 300\n",
      "        nms_score_threshold: 0.050000001\n",
      "        nms_iou_threshold: 0.30000001\n",
      "        matched_threshold: 0.2\n",
      "        unmatched_threshold: 0.15000001\n",
      "        class_name: \"pedestrian\"\n",
      "      }\n",
      "      class_settings {\n",
      "        anchor_generator_range {\n",
      "          sizes: 2.8460939\n",
      "          sizes: 10.247781\n",
      "          sizes: 3.4400492\n",
      "          anchor_ranges: -49.599998\n",
      "          anchor_ranges: -9.5999985\n",
      "          anchor_ranges: -0.30000001\n",
      "          anchor_ranges: 9.5999985\n",
      "          anchor_ranges: 9.5999985\n",
      "          anchor_ranges: -0.30000001\n",
      "          rotations: 0.0\n",
      "          rotations: 1.5700001\n",
      "        }\n",
      "        region_similarity_calculator {\n",
      "          nearest_iou_similarity {\n",
      "          }\n",
      "        }\n",
      "        nms_pre_max_size: 1000\n",
      "        nms_post_max_size: 300\n",
      "        nms_score_threshold: 0.050000001\n",
      "        nms_iou_threshold: 0.30000001\n",
      "        matched_threshold: 0.5\n",
      "        unmatched_threshold: 0.30000001\n",
      "        class_name: \"truck\"\n",
      "      }\n",
      "      sample_positive_fraction: -1.0\n",
      "      sample_size: 512\n",
      "      assign_per_class: true\n",
      "    }\n",
      "    post_center_limit_range: -49.599998\n",
      "    post_center_limit_range: -9.5999985\n",
      "    post_center_limit_range: -10.0\n",
      "    post_center_limit_range: 9.5999985\n",
      "    post_center_limit_range: 9.5999985\n",
      "    post_center_limit_range: 10.0\n",
      "    direction_offset: 0.77999997\n",
      "    sin_error_factor: 1.0\n",
      "    num_direction_bins: 2\n",
      "  }\n",
      "}\n",
      "train_input_reader {\n",
      "  batch_size: 3\n",
      "  dataset {\n",
      "    kitti_info_path: \"/home/itiv/Desktop/lyft-dataset//infos_train.pkl\"\n",
      "    kitti_root_path: \"/home/itiv/Desktop/lyft-dataset/\"\n",
      "    dataset_class_name: \"NuScenesDataset\"\n",
      "  }\n",
      "  preprocess {\n",
      "    max_number_of_voxels: 20000\n",
      "    groundtruth_localization_noise_std: 0.0\n",
      "    groundtruth_localization_noise_std: 0.0\n",
      "    groundtruth_localization_noise_std: 0.0\n",
      "    groundtruth_rotation_uniform_noise: 0.0\n",
      "    groundtruth_rotation_uniform_noise: 0.0\n",
      "    global_rotation_uniform_noise: 0.0\n",
      "    global_rotation_uniform_noise: 0.0\n",
      "    global_scaling_uniform_noise: 0.94999999\n",
      "    global_scaling_uniform_noise: 1.05\n",
      "    global_translate_noise_std: 0.2\n",
      "    global_translate_noise_std: 0.2\n",
      "    global_translate_noise_std: 0.2\n",
      "    num_workers: 3\n",
      "    anchor_area_threshold: -1.0\n",
      "    remove_points_after_sample: true\n",
      "    groundtruth_drop_max_keep_points: 15\n",
      "    global_random_rotation_range_per_object: 0.0\n",
      "    global_random_rotation_range_per_object: 0.0\n",
      "    database_sampler {\n",
      "      database_info_path: \"/home/itiv/Desktop/lyft-dataset//infos_train.pkl\"\n",
      "      global_random_rotation_range_per_object: 0.0\n",
      "      global_random_rotation_range_per_object: 0.0\n",
      "      rate: 1.0\n",
      "    }\n",
      "    random_flip_x: true\n",
      "    random_flip_y: true\n",
      "    sample_importance: 1.0\n",
      "  }\n",
      "}\n",
      "train_config {\n",
      "  optimizer {\n",
      "    adam_optimizer {\n",
      "      learning_rate {\n",
      "        one_cycle {\n",
      "          lr_max: 0.003\n",
      "          moms: 0.94999999\n",
      "          moms: 0.85000002\n",
      "          div_factor: 10.0\n",
      "          pct_start: 0.40000001\n",
      "        }\n",
      "      }\n",
      "      weight_decay: 0.0099999998\n",
      "    }\n",
      "    fixed_weight_decay: true\n",
      "  }\n",
      "  steps: 200000\n",
      "  steps_per_eval: 1000000\n",
      "  save_checkpoints_secs: 1800\n",
      "  save_summary_steps: 10\n",
      "  loss_scale_factor: -1.0\n",
      "  clear_metrics_every_epoch: true\n",
      "}\n",
      "eval_input_reader {\n",
      "  batch_size: 1\n",
      "  dataset {\n",
      "    kitti_info_path: \"/home/itiv/Desktop/lyft-dataset//infos_val.pkl\"\n",
      "    kitti_root_path: \"/home/itiv/Desktop/lyft-dataset/\"\n",
      "    dataset_class_name: \"NuScenesDataset\"\n",
      "  }\n",
      "  preprocess {\n",
      "    max_number_of_voxels: 30000\n",
      "    num_workers: 1\n",
      "    anchor_area_threshold: -1.0\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "starting from: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORKER 0 seed: 1584616813\n",
      "WORKER 1 seed: 1584616814\n",
      "WORKER 2 seed: 1584616815\n",
      "\n",
      "runtime.step=50, runtime.steptime=0.2582, runtime.voxel_gene_time=0.007626, runtime.prep_time=0.01445, loss.cls_loss=416.1, loss.cls_loss_rt=6.691, loss.loc_loss=2.033, loss.loc_loss_rt=1.86, loss.loc_elem=[0.2136, 0.05881, 0.2341, 0.07719, 0.1219, 0.1151, 0.1096], loss.cls_pos_rt=5.452, loss.cls_neg_rt=1.239, loss.dir_rt=0.5552, rpn_acc=0.7726, misc.num_vox=44326, misc.num_pos=9, misc.num_neg=5089, misc.num_anchors=5100, misc.lr=0.0003, misc.mem_usage=45.2\n",
      "\n",
      "runtime.step=100, runtime.steptime=0.1821, runtime.voxel_gene_time=0.008033, runtime.prep_time=0.0149, loss.cls_loss=208.7, loss.cls_loss_rt=0.6953, loss.loc_loss=1.758, loss.loc_loss_rt=1.658, loss.loc_elem=[0.1408, 0.04813, 0.2803, 0.0541, 0.04867, 0.08038, 0.1764], loss.cls_pos_rt=0.1889, loss.cls_neg_rt=0.5064, loss.dir_rt=0.6299, rpn_acc=0.8856, misc.num_vox=45443, misc.num_pos=10, misc.num_neg=5088, misc.num_anchors=5100, misc.lr=0.0003, misc.mem_usage=45.2\n",
      "\n",
      "runtime.step=150, runtime.steptime=0.1797, runtime.voxel_gene_time=0.007498, runtime.prep_time=0.01459, loss.cls_loss=139.4, loss.cls_loss_rt=0.7746, loss.loc_loss=1.599, loss.loc_loss_rt=1.271, loss.loc_elem=[0.1656, 0.07303, 0.09553, 0.03213, 0.0574, 0.05663, 0.1553], loss.cls_pos_rt=0.3902, loss.cls_neg_rt=0.3844, loss.dir_rt=0.6706, rpn_acc=0.9232, misc.num_vox=34840, misc.num_pos=7, misc.num_neg=5093, misc.num_anchors=5100, misc.lr=0.0003, misc.mem_usage=46.3\n",
      "\n",
      "runtime.step=200, runtime.steptime=0.1822, runtime.voxel_gene_time=0.009359, runtime.prep_time=0.01747, loss.cls_loss=104.7, loss.cls_loss_rt=0.6234, loss.loc_loss=1.518, loss.loc_loss_rt=1.515, loss.loc_elem=[0.165, 0.1076, 0.1901, 0.06761, 0.05493, 0.06297, 0.1092], loss.cls_pos_rt=0.194, loss.cls_neg_rt=0.4294, loss.dir_rt=0.6883, rpn_acc=0.942, misc.num_vox=39126, misc.num_pos=11, misc.num_neg=5083, misc.num_anchors=5100, misc.lr=0.0003, misc.mem_usage=47.5\n",
      "\n",
      "runtime.step=250, runtime.steptime=0.1855, runtime.voxel_gene_time=0.007511, runtime.prep_time=0.0188, loss.cls_loss=83.88, loss.cls_loss_rt=0.6787, loss.loc_loss=1.467, loss.loc_loss_rt=1.461, loss.loc_elem=[0.1243, 0.07095, 0.1626, 0.03887, 0.0297, 0.04386, 0.2602], loss.cls_pos_rt=0.1985, loss.cls_neg_rt=0.4802, loss.dir_rt=0.5933, rpn_acc=0.9533, misc.num_vox=35910, misc.num_pos=6, misc.num_neg=5092, misc.num_anchors=5100, misc.lr=0.0003001, misc.mem_usage=48.4\n",
      "\n",
      "runtime.step=300, runtime.steptime=0.1873, runtime.voxel_gene_time=0.005427, runtime.prep_time=0.0124, loss.cls_loss=70.0, loss.cls_loss_rt=0.5744, loss.loc_loss=1.414, loss.loc_loss_rt=0.9944, loss.loc_elem=[0.1191, 0.05832, 0.1084, 0.05428, 0.05672, 0.06865, 0.03169], loss.cls_pos_rt=0.2149, loss.cls_neg_rt=0.3594, loss.dir_rt=0.6384, rpn_acc=0.9608, misc.num_vox=32901, misc.num_pos=10, misc.num_neg=5084, misc.num_anchors=5100, misc.lr=0.0003001, misc.mem_usage=48.5\n",
      "\n",
      "runtime.step=350, runtime.steptime=0.176, runtime.voxel_gene_time=0.007211, runtime.prep_time=0.01601, loss.cls_loss=60.08, loss.cls_loss_rt=0.5805, loss.loc_loss=1.388, loss.loc_loss_rt=1.192, loss.loc_elem=[0.09626, 0.07041, 0.2278, 0.02859, 0.06007, 0.05125, 0.06168], loss.cls_pos_rt=0.1661, loss.cls_neg_rt=0.4145, loss.dir_rt=0.8229, rpn_acc=0.9662, misc.num_vox=32300, misc.num_pos=16, misc.num_neg=5075, misc.num_anchors=5100, misc.lr=0.0003001, misc.mem_usage=48.2\n"
     ]
    }
   ],
   "source": [
    "##### automated Training and testing....\n",
    "import numpy as np\n",
    "from second.protos import pipeline_pb2\n",
    "from google.protobuf import text_format\n",
    "\n",
    "\n",
    "\n",
    "models = ['smallv2','middlev2','largev2']\n",
    "step_size = 200000\n",
    "eval_scenes = [30,104,108,130,141,142]\n",
    "\n",
    "for epoch in range(4):\n",
    "    for model in models:\n",
    "        try:\n",
    "            LYFT_TRAINVAL_DATASET_ROOT = '/home/itiv/Desktop/lyft-dataset/'\n",
    "            config_path = 'Lyft-Detector/second.pytorch/second/configs/nuscenes/all.pp.lowa_'+model+'_range.config'\n",
    "            model_path = 'Lyft-Detector/second.pytorch/second/model/model_'+model+'_range'\n",
    "\n",
    "\n",
    "            ## set paths in config file...\n",
    "\n",
    "            step = step_size*(epoch+1)\n",
    "\n",
    "\n",
    "            config = pipeline_pb2.TrainEvalPipelineConfig()\n",
    "            with open(config_path, \"r\") as f:\n",
    "                proto_str = f.read()\n",
    "                text_format.Merge(proto_str, config)\n",
    "            config.train_input_reader.dataset.kitti_info_path = LYFT_TRAINVAL_DATASET_ROOT+\"/infos_train.pkl\"\n",
    "            config.train_input_reader.dataset.kitti_root_path = LYFT_TRAINVAL_DATASET_ROOT\n",
    "\n",
    "            config.train_input_reader.preprocess.database_sampler.database_info_path = LYFT_TRAINVAL_DATASET_ROOT+\"/infos_train.pkl\"\n",
    "\n",
    "            config.eval_input_reader.dataset.kitti_info_path = LYFT_TRAINVAL_DATASET_ROOT+\"/infos_val.pkl\"\n",
    "            config.eval_input_reader.dataset.kitti_root_path = LYFT_TRAINVAL_DATASET_ROOT\n",
    "            config.train_config.steps = step\n",
    "\n",
    "            config_text = text_format.MessageToString(config)\n",
    "            with open(config_path, \"w\") as f:\n",
    "                f.write(config_text)\n",
    "\n",
    "            print('Paths set in config file')\n",
    "\n",
    "\n",
    "            ##start training\n",
    "\n",
    "            !python ./Lyft-Detector/second.pytorch/second/pytorch/train.py train --config_path=$config_path --model_dir=$model_path\n",
    "\n",
    "\n",
    "            net = '/model_'+model+'_range/voxelnet-'+str(step)+'.tckpt'\n",
    "\n",
    "\n",
    "            ##evaluate\n",
    "            iou_arr = []\n",
    "            gt_ids = []\n",
    "            det_ids = []\n",
    "            id_counter = 0\n",
    "            accs = []\n",
    "            for scene_num in eval_scenes:\n",
    "                #create_pkl(scene_num)\n",
    "                print(scene_num)\n",
    "                set_config_file(scene_num,model)\n",
    "                track(scene_num,model,net)\n",
    "\n",
    "                det_dict = loadDetectionDict(scene_num,model)\n",
    "                gt_dict = load_ground_truth_data(scene_num,model)\n",
    "                scene_iou_arr,scene_gt_ids,scene_det_ids,id_counter_val = get_iou_for_scene(gt_dict,det_dict,id_counter)\n",
    "                id_counter = id_counter_val\n",
    "                det_ids.extend(scene_det_ids)\n",
    "                gt_ids.extend(scene_gt_ids)\n",
    "                iou_arr.extend(scene_iou_arr)\n",
    "                print(f'finished analyzing scene: {scene_num}')\n",
    "\n",
    "\n",
    "            acc = get_MOTA_Acc(iou_arr,gt_ids,det_ids)\n",
    "\n",
    "            print('creating metric')\n",
    "\n",
    "            mh = mm.metrics.create()\n",
    "            summary = mh.compute(acc, metrics=mm.metrics.motchallenge_metrics, name=model)\n",
    "            print(summary)\n",
    "\n",
    "            now = datetime.now() # current date and time\n",
    "            date_time = now.strftime(\"%m/%d/%Y, %H:%M:%S\")\n",
    "\n",
    "            with open('EvaluationResults.txt', \"a\") as resultFile:\n",
    "                resultFile.write(\"\\n #### \"+str(date_time)+' ### single scene analysis \\n')\n",
    "                resultFile.write(\"Net: \"+net+' \\n')\n",
    "\n",
    "\n",
    "            summary.to_csv('EvaluationResults.txt', sep='\\t', mode='a',index=True)\n",
    "        except Exception as e: \n",
    "            print(e)\n",
    "            print('Error....skipping')   \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "models = ['small','middle','large']\n",
    "\n",
    "\n",
    "for epoch in range(4):\n",
    "    for model in models:\n",
    "        try:\n",
    "            LYFT_TRAINVAL_DATASET_ROOT = '/home/itiv/Desktop/lyft-dataset/'\n",
    "            config_path = 'Lyft-Detector/second.pytorch/second/configs/nuscenes/all.pp.lowa_'+model+'_range.config'\n",
    "            model_path = 'Lyft-Detector/second.pytorch/second/model/model_'+model+'_range'\n",
    "\n",
    "\n",
    "            ## set paths in config file...\n",
    "\n",
    "            step = step_size*(epoch+1)\n",
    "\n",
    "\n",
    "            config = pipeline_pb2.TrainEvalPipelineConfig()\n",
    "            with open(config_path, \"r\") as f:\n",
    "                proto_str = f.read()\n",
    "                text_format.Merge(proto_str, config)\n",
    "            config.train_input_reader.dataset.kitti_info_path = LYFT_TRAINVAL_DATASET_ROOT+\"/infos_train.pkl\"\n",
    "            config.train_input_reader.dataset.kitti_root_path = LYFT_TRAINVAL_DATASET_ROOT\n",
    "\n",
    "            config.train_input_reader.preprocess.database_sampler.database_info_path = LYFT_TRAINVAL_DATASET_ROOT+\"/infos_train.pkl\"\n",
    "\n",
    "            config.eval_input_reader.dataset.kitti_info_path = LYFT_TRAINVAL_DATASET_ROOT+\"/infos_val.pkl\"\n",
    "            config.eval_input_reader.dataset.kitti_root_path = LYFT_TRAINVAL_DATASET_ROOT\n",
    "            config.train_config.steps = step\n",
    "\n",
    "            config_text = text_format.MessageToString(config)\n",
    "            with open(config_path, \"w\") as f:\n",
    "                f.write(config_text)\n",
    "\n",
    "            print('Paths set in config file')\n",
    "\n",
    "\n",
    "            ##start training\n",
    "\n",
    "            !python ./Lyft-Detector/second.pytorch/second/pytorch/train.py train --config_path=$config_path --model_dir=$model_path\n",
    "\n",
    "\n",
    "            net = '/model_'+model+'_range/voxelnet-'+str(step)+'.tckpt'\n",
    "\n",
    "\n",
    "            ##evaluate\n",
    "            iou_arr = []\n",
    "            gt_ids = []\n",
    "            det_ids = []\n",
    "            id_counter = 0\n",
    "            accs = []\n",
    "            for scene_num in eval_scenes:\n",
    "                #create_pkl(scene_num)\n",
    "                print(scene_num)\n",
    "                set_config_file(scene_num,model)\n",
    "                track(scene_num,model,net)\n",
    "\n",
    "                det_dict = loadDetectionDict(scene_num,model)\n",
    "                gt_dict = load_ground_truth_data(scene_num,model)\n",
    "                scene_iou_arr,scene_gt_ids,scene_det_ids,id_counter_val = get_iou_for_scene(gt_dict,det_dict,id_counter)\n",
    "                id_counter = id_counter_val\n",
    "                det_ids.extend(scene_det_ids)\n",
    "                gt_ids.extend(scene_gt_ids)\n",
    "                iou_arr.extend(scene_iou_arr)\n",
    "                print(f'finished analyzing scene: {scene_num}')\n",
    "\n",
    "\n",
    "            print('acc')\n",
    "            acc = get_MOTA_Acc(iou_arr,gt_ids,det_ids)\n",
    "\n",
    "            print('creating metric')\n",
    "\n",
    "            mh = mm.metrics.create()\n",
    "            summary = mh.compute(acc, metrics=mm.metrics.motchallenge_metrics, name=model)\n",
    "            print(summary)\n",
    "\n",
    "            now = datetime.now() # current date and time\n",
    "            date_time = now.strftime(\"%m/%d/%Y, %H:%M:%S\")\n",
    "\n",
    "            with open('EvaluationResults.txt', \"a\") as resultFile:\n",
    "                resultFile.write(\"\\n #### \"+str(date_time)+' ### single scene analysis \\n')\n",
    "                resultFile.write(\"Net: \"+net+' \\n')\n",
    "\n",
    "\n",
    "            summary.to_csv('EvaluationResults.txt', sep='\\t', mode='a',index=True)\n",
    "        except Exception as e: \n",
    "            print(e)\n",
    "            print('Error....skipping')        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:lyft_detector]",
   "language": "python",
   "name": "conda-env-lyft_detector-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
